---
title: "Ch13_Adventures_in_Covariance"
author: "Michael Rose"
date: "July 4, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
library(MASS)
library(ellipse)
```

# Adventures in Covariance

The essence of the general **varying effects** strategy is that any batch of parameters with *exchangeable* index values can and probably should be pooled. Exchangeable means that index values have no true ordering, because they are arbitrary labels. 

In our models, theres nothing special about intercepts; slopes can also vary by unit in our data, and pooling information among them makes better use of the data. So our golem should be programmed to model both the population of intercepts and the population of slopes. 

In this chapter we will see how to specify **varying slopes** in combination with the varying intercepts of the previous chapter. This will enable pooling that will improve estimates of how different units respond to or are influenced by predictor variables. 

# 13.1 | Varying slopes by construction 

The robot should pool information across intercepts and slopes by modeling the joint population of intercepts and slopes, which means modeling their covariance. Instead of having two independent Gaussian distributions of intercepts and of slopes, the robot can do better by assigning a two dimensional Gaussian distribution to both the intercepts (first dimension) and slopes (second dimension). 

## 13.1.1 | Simulate the population

```{r}
# average morning wait time 
a <- 3.5 

# average difference afternoon wait time 
b <- (-1) 

# std dev in intercepts 
sigma_a <- 1

# std dev in slopes 
sigma_b <- 0.5 

# correlation between intercepts and slopes 
rho <- (-0.7)

# build these values into a 2d multivariate gaussian  

# build a vector of 2 means
Mu <- c(a, b)

# build matrix of variances 
# option 1: build the entire cov matrix directly. The output will look odd due to how R fills rows
cov_ab <- sigma_a * sigma_b * rho 
Sigma <- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2)

# option 2:
sigmas <- c(sigma_a, sigma_b) # std devs 
Rho <- matrix(c(1, rho, rho, 1), nrow = 2) # correlation matrix 

# matrix multiply to get covariance matrix 
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)

# define number of cafes 
N_cafes <- 20 

# simulate their properties by sampling randomly from the MV Gaussian distribution 
set.seed(5) 
vary_effects <- mvrnorm(N_cafes, Mu, Sigma)

# get coefficients from vary_effects 
a_cafe <- vary_effects[,1] 
b_cafe <- vary_effects[,2] 

# plot 
plot(a_cafe, b_cafe, col = rangi2, xlab = "intercepts (a_cafe)", ylab = "slopes (b_cafe)")

# overlay population distribution 
for (l in c(0.1, 0.3, 0.5, 0.8, 0.99)){
  lines(ellipse(Sigma, centre = Mu, level = l), col = col.alpha("black", 0.2))
}
```

In the plot above: 
20 cafes sampled from a multivariate norm. The horizontal axis is the intercept (average morning wait) for each cafe. The vertical axis is the slope (average difference between afternoon and morning wait) for each cafe. The gray ellipses illustrate the MV Gaussian population of intercepts and slopes 

## 13.1.2 | Simulate Observations 

Above we simulated individual cafes and their average properties. Now we need to simulate our robot visiting these cafes and collecting data. The code below simulates 10 visits to each cafe - 5 in the morning and 5 in the afternoon. The robot records the wait time during each visit and then combines it all into a dataframe

```{r}
N_visits <- 10
afternoon <- rep(0:1, N_visits * N_cafes / 2) 
cafe_id <- rep(1:N_cafes, each = N_visits)
mu <- a_cafe[cafe_id] + b_cafe[cafe_id] * afternoon 
sigma <- 0.5 # std dev within cafes 
wait <- rnorm(N_visits * N_cafes, mu, sigma) 
d <- data.frame(cafe = cafe_id, afternoon = afternoon, wait = wait)
d
```

## 13.1.3 | The varying slopes model 

```{r}
# example for LKJcorr prior 
R <- rlkjcorr(1e4, K = 2, eta = 2)
dens(R[,1,2], xlab = "correlation")

# fit our robot model 
m13.1 <- map2stan(
  alist(
    wait ~ dnorm(mu, sigma),
    mu <- a_cafe[cafe] + b_cafe[cafe] * afternoon, 
    c(a_cafe, b_cafe)[cafe] ~ dmvnorm2(c(a, b), sigma_cafe, Rho), 
    a ~ dnorm(0, 10), 
    b ~ dnorm(0, 10), 
    sigma_cafe ~ dcauchy(0, 2), 
    sigma ~ dcauchy(0, 2), 
    Rho ~ dlkjcorr(2)
  ), data = d, iter = 5000, warmup = 2000, chains = 2, cores = 4
)
```

```{r}
# pull samples from posterior 
post <- extract.samples(m13.1) 
dens(post$Rho[,1,2])
```

Next we consider shrinkage. The multilevel model estimtes posterior distributions for intercepts and slopes of each cafe. The inferred correlation between these varying effects was used to pool information among them, as well as how the inferred variation among slopes pools information among them. 

To see the consequences of this shrinkage (adaptive regularization), lets plot the posterior mean varying effects 

```{r}
# compute unpooled estimates directly from data 
a1 <- sapply(1:N_cafes, function(i) mean(wait[cafe_id == i & afternoon == 0]))
b1 <- sapply(1:N_cafes, function(i) mean(wait[cafe_id == i & afternoon == 1])) - a1 

# extract posterior means of partially pooled estimates 
post <- extract.samples(m13.1) 
a2 <- apply(post$a_cafe, 2, mean) 
b2 <- apply(post$b_cafe, 2, mean) 

# plot both and connect with lines 
plot(a1, b1, xlab = "intercept", ylab = "slope", pch = 16, col = rangi2, ylim = c(min(b1) - 0.1, max(b1) + 0.1), xlim = c(min(a1) - 0.1, max(a1) + 0.1)) 
points(a2, b2, pch = 1)
for (i in 1:N_cafes) lines(c(a1[i], a2[i]), c(b1[i], b2[i]))

# superimpose the contours of the population 

# compute posterior mean bivariate gaussian 
Mu_est <- c(mean(post$a), mean(post$b)) 
rho_est <- mean(post$Rho[,1,2]) 
sa_est <- mean(post$sigma_cafe[,1])
sb_est <- mean(post$sigma_cafe[,2])
cov_ab <- sa_est * sb_est * rho_est 
Sigma_est <- matrix(c(sa_est^2, cov_ab, cov_ab, sb_est^2), ncol = 2)

# draw contours 
for (l in c(0.1, 0.3, 0.5, 0.8, 0.99)){
  lines(ellipse(Sigma_est, centre = Mu_est, level = l), col = col.alpha("black", 0.2))
}
```

In the plot above: 
Raw unpooled intercepts and slopes (filled blue) compared to partially pooled posterior means (open circles). The gray contours show the inferred population of varying effects. Each line connecting the dots belongs to the same cafe. Blue points farther from the center experience more shrinkage, because they are less plausible, given the inferred population.

# 13.2 | Example: Admission Decisions and Gender 

In our previous UCBadmits example, we left some information on the floor by not using varying effects to pool information across departments. As a consequence, we probably overfit the smaller departments. We also ignored variation across departments in how they treated male and female applicants. 

```{r}
data(UCBadmit)
d <- UCBadmit
d$male <- ifelse(d$applicant.gender == "male", 1, 0) 
d$dept_id <- coerce_index(d$dept)
```

## 13.2.1 | Varying Intercepts 

Here's the model with varying intercepts:
$A_i \sim \mathrm{Binomial}(n_i, p_i)$  Likelihood 
$\mathrm{logit}(p_i) = \alpha_{dept[i]} + \beta m_i$  Linear Model 
$\alpha_{dept} \sim \mathrm{Normal}(\alpha, \sigma)$  Prior for varying intercepts 
$\alpha \sim \mathrm{Normal}(0, 10)$  Prior for alpha 
$\beta \sim \mathrm{Normal}(0, 1)$ Prior for beta 
$\sigma \sim \mathrm{HalfCauchy}(0, 2)$ Prior for sigma 

```{r}
# fit varying intercepts model 
m13.2 <- map2stan(
  alist(
    admit ~ dbinom(applications, p), 
    logit(p) <- a_dept[dept_id] + bm * male, 
    a_dept[dept_id] ~ dnorm(a, sigma_dept), 
    a ~ dnorm(0, 10), 
    bm ~ dnorm(0, 1), 
    sigma_dept ~ dcauchy(0, 2)
  ), data = d, warmup = 500, iter = 4500, chains = 3, cores = 4
)

precis(m13.2, depth = 2)
```

## 13.2.2 | Varying effects of being male 

Now lets consider the variation in gender bias among departments. In the previous model there isn't much evidence of gender bias, but what if we allow the effect of an applicant's being male to vary in the same way we already allowed the overall rate of admission to vary? This is the varying slopes model in this context. 

One extra feature of varying slopes that will arise here is that since there is substantial imbalance in sample size across departments and the number of male and female applications they recieved, pooling will be stronger for those cases with fewer applications. 

Here is the varying slopes model :

$A_i \sim \mathrm{Binomial}(n_i, p_i)$  Likelihood 
$\mathrm{logit}(p_i) = \alpha_{dept[i]} + \beta_{dept[i]} m_i$  Linear Model 
$\begin{bmatrix} \alpha_{dept} \\\beta_{dept} \end{bmatrix} \sim \mathrm{MVNormal}(\begin{bmatrix}  \alpha \\ \beta \end{bmatrix}, S)$ joint prior for varying effects
$S = (\begin {matrix} \sigma_{\alpha} 0 \\ 0 \sigma_{\beta}  \end{matrix}) R (\begin {matrix} \sigma_{\alpha} 0 \\ 0 \sigma_{\beta}  \end{matrix})$
$\alpha \sim \mathrm{Normal}(0, 10)$  Prior for alpha 
$\beta \sim \mathrm{Normal}(0, 1)$ Prior for beta 
$(\sigma_\alpha, \sigma_\beta) \sim \mathrm{HalfCauchy}(0, 2)$ Prior for sigma
$R \sim \mathrm{LKJcorr}(2)$ Prior for correlation matrix 

```{r}
# fit varying slopes model 
m13.3 <- map2stan(
  alist(
        admit ~ dbinom(applications, p),
        logit(p) <- a_dept[dept_id] +
        bm_dept[dept_id]*male,
        c(a_dept,bm_dept)[dept_id] ~ dmvnorm2( c(a,bm) , sigma_dept , Rho ),
        a ~ dnorm(0,10),
        bm ~ dnorm(0,1),
        sigma_dept ~ dcauchy(0,2),
        Rho ~ dlkjcorr(2)
),
data=d, warmup=1000, iter=5000, chains=4, cores=3)

plot(precis(m13.3, pars = c("a_dept", "bm_dept"), depth = 2))

```

In the graph above, we notice that the intercepts are all over the place, but the slopes are relatively close to 0. This reflects that the departments varied a lot, but they neither discriminated much between male and female applicants nor varied much in how much they discriminated. Of the department slopes, the only ones with any noticeable bias was depts 1 and 2. 

Since these two depts have the largest intercepts, we should loomk at the estimated correlation between intercepts and slopes next, as well as the 2d shrinkage it induces. 

## 13.2.4 | Model Comparison 

```{r}
# fit a model that ignores gender for comparison 
m13.4 <- map2stan(
  alist(
    admit ~ dbinom(applications, p), 
    logit(p) <- a_dept[dept_id], 
    a_dept[dept_id] ~ dnorm(a, sigma_dept), 
    a ~ dnorm(0, 10), 
    sigma_dept ~ cauchy(0, 2)
  ), data = d, warmup = 500, iter = 4500, chains = 4
)

compare(m13.2, m13.3, m13.4)
```

Notice how the model with departments and the varying slopes got 99% of the weight

# 13.3 | Example: Cross-classified Chimpanzees with Varying Slopes 

In this example we construct a model with more than 2 varying effects - varying intercepts plus more than one varying slope - as well as more than one type of cluster. 

This example will be used to emphasize the importance of **non-centered parameterization** for some multilevel models. 
The way we write down the mathematical model is our parameterization. Mathematically, they are all equivalent, but to the MCMC engine they are not.

```{r}
data(chimpanzees)
d <- chimpanzees
d$recipient <- NULL 
d$block_id <- d$block

# fit model 
m13.6 <- map2stan(
  alist(
    # likelihood 
    pulled_left ~ dbinom(1, p), 
    # linear models 
    logit(p) <- A + (BP + BPC * condition)*prosoc_left, 
    A <- a + a_actor[actor] + a_block[block_id], 
    BP <- bp + bp_actor[actor] + bp_block[block_id], 
    BPC <- bpc + bpc_actor[actor] + bpc_block[block_id], 
    # adaptive priors 
    c(a_actor, bp_actor, bpc_actor)[actor] ~ dmvnorm2(0, sigma_actor, Rho_actor), 
    c(a_block, bp_block, bpc_block)[block_id] ~ dmvnorm2(0, sigma_block, Rho_block), 
    # fixed priors 
    c(a, bp, bpc) ~ dnorm(0, 1), 
    sigma_actor ~ dcauchy(0, 2), 
    sigma_block ~ dcauchy(0, 2), 
    Rho_actor ~ dlkjcorr(4), 
    Rho_block ~ dlkjcorr(4)
  ), data = d, iter = 5000, warmup = 1000, chains = 3, cores = 3
)


```

Sometimes brute force doesn't converge. In this case, we can use non centered parameterization. Heres the model with dvnormNC instead of dmvnorm2 

```{r}
m13.6NC <- map2stan(
alist(
pulled_left ~ dbinom(1,p),
logit(p) <- A + (BP + BPC*condition)*prosoc_left,
A <- a + a_actor[actor] + a_block[block_id],
BP <- bp + bp_actor[actor] + bp_block[block_id],
BPC <- bpc + bpc_actor[actor] + bpc_block[block_id],
# adaptive NON-CENTERED priors
c(a_actor,bp_actor,bpc_actor)[actor] ~
dmvnormNC(sigma_actor,Rho_actor),
c(a_block,bp_block,bpc_block)[block_id] ~
dmvnormNC(sigma_block,Rho_block),
c(a,bp,bpc) ~ dnorm(0,1),
sigma_actor ~ dcauchy(0,2),
sigma_block ~ dcauchy(0,2),
Rho_actor ~ dlkjcorr(4),
Rho_block ~ dlkjcorr(4)
) , data=d , iter=5000 , warmup=1000 , chains=3 , cores=3 )
```

```{r}
# extract n_eff values for each model 
neff_c <- precis(m13.6, 2)@output$n_eff 
neff_nc <- precis(m13.6NC,2)@output$n_eff 

# plot distributions 
boxplot(list('m13.6' = neff_c, 'm13.6NC' = neff_nc), ylab = "effective samples", xlab = "model")
```

As we can see in the plot above, the non centered version of the model got a lot more effective samples, giving us a better picture of our posterior. 

```{r}
# inspect the standard deviation to get a sense of how aggressively the varying effects are being regularized 
precis(m13.6NC, depth = 2, pars = c("sigma_actor", "sigma_block"))
```

In the table above: 
  - The [1] index in each vector is the varying intercept standard deviation 
  - The [2] and [3] indexes are slopes
  
```{r}
# efficient parameterization of m13.6 to eliminate divergent chains 
m13.6nc1 <- map2stan(
  alist(
    pulled_left ~ dbinom(1, p), 
    # linear models 
    logit(p) <- A + (BP + BPC * condition) * prosoc_left, 
    A <- a + za_actor[actor]*sigma_actor[1] +
         za_block[block_id]*sigma_block[1],
    BP <- bp + zbp_actor[actor]*sigma_actor[2] +
          zbp_block[block_id]*sigma_block[2],
    BPC <- bpc + zbpc_actor[actor]*sigma_actor[3] +
           zbpc_block[block_id]*sigma_block[3],
    # adaptive priors
    c(za_actor,zbp_actor,zbpc_actor)[actor] ~ dmvnorm(0,Rho_actor),
    c(za_block,zbp_block,zbpc_block)[block_id] ~ dmvnorm(0,Rho_block),
    # fixed priors
    c(a,bp,bpc) ~ dnorm(0,1),
    sigma_actor ~ dcauchy(0,2),
    sigma_block ~ dcauchy(0,2),
    Rho_actor ~ dlkjcorr(4),
    Rho_block ~ dlkjcorr(4)
) ,
data=d ,
start=list( sigma_actor=c(1,1,1), sigma_block=c(1,1,1) ),
constraints=list( sigma_actor="lower=0", sigma_block="lower=0" ),
types=list( Rho_actor="corr_matrix", Rho_block="corr_matrix" ),
iter=5000 , warmup=1000 , chains=3 , cores=3 )
```

# Continuous Categories and the Gaussian Process 

There is a way to apply the varying effects approach to continuous categories, allowing us to have a unique intercept for any age (or slope) while still regarding age as a continuous dimension in which similar ages have more similar intercepts. This general approach is known as **Gaussian Process Regression**. 

The general purpose is to define some dimension along which cases differ. This might be individual differences in age or location, and then we measure the distances between each pair of cases. What the model does it estimate a function for the covariance between pairs of cases at different distances. This covariance function provides one continuous category generalization of the varying effects approach. 

## 13.4.1 | Example: Spatial Autocorrelation in Oceanic Tools 

In this example we are looking at the relationships between tool creation and distance for oceanic islands. This is a classic setting for Gaussian process regression. 
First we will define a distance matrix among the societies. Then we can estimate how similarity in tool counts depends upon geographic distance. Then we will see how to simultaneously incorporate ordinary predictors, so that the covariation among societies with distance will both control for and be controlled by other factors that influence technology. 

```{r}
# load data 
data(islandsDistMatrix)
Dmat <- islandsDistMatrix

# display short column names so it fits on screen 
colnames(Dmat) <- c("Ml", "Ti", "SC", "Ya", "Fi", "Tr", "Ch", "Mn", "To", "Ha")
round(Dmat, 1)
```

```{r}
# linear 
curve(exp(-1 * x), from = 0, to = 4, lty = 2, xlab = "distance", ylab = "correlation") 

# squared 
curve(exp(-1 * x^2))
```

Here is our full Gaussian Process model: 
$T_i \sim \mathrm{Poisson}(\lambda_i)$ 
$\log(\lambda_i) = \alpha + \gamma_{society[i]} + \beta_p \log(P_i)$ 
$\gamma \sim \mathrm{MVNormal}([0, ..., 0], K)$ 
$K_{ij} = \eta^2 \exp(-\rho^2 D_{ij}^2) + \delta_{ij}(0.01)$ 
$\alpha \sim \mathrm{Normal}(0, 10)$ 
$\beta_p \sim \mathrm{Normal}(0, 1)$ 
$\eta^2 \sim \mathrm{HalfCauchy}(0, 1)$
$\rho^2 \sim \mathrm{HalfCauchy}(0, 1)$

```{r}
# load the data 
data(Kline2)
d <- Kline2
d$society <- 1:10 # index observations 

# fit model 
m13.7 <- map2stan(
  alist(
    total_tools ~ dpois(lambda), 
    log(lambda) <- a + g[society] + bp*logpop, 
    g[society] ~ GPL2(Dmat, etasq, rhosq, 0.01), 
    a ~ dnorm(0, 10), 
    bp ~ dnorm(0, 1), 
    etasq ~ dcauchy(0, 1), 
    rhosq ~ dcauchy(0, 1)
  ), data = list(total_tools = d$total_tools,
                 logpop = d$logpop, 
                 society = d$society, 
                 Dmat = islandsDistMatrix), 
  warmup = 2000, iter = 1e4, chains = 4, cores = 4
)

precis(m13.7, depth = 2)

```

First we can note that the coefficient for log population, bp, is essentially the same as before we added all the gaussian process stuff. This suggests that its hard to explain all of the association between tool counts and population as a side effect of geographic contact. 

Those g parameters are the gaussian process varying intercepts for each society. Like a and bp, they are on the log-count scale. 

In order to understand the parameters that describe the covariance with distance, rhosq and etasq, we will want to plot the function they imply. We can get a sense of this distribution of functions by plotting them .

```{r}
# we will sample 100 from the posterior and display them along with the posterior median. 
# we choose median because the densities for rhosq and etasq are skewed. We see this from the precis output

# get samples 
post <- extract.samples(m13.7) 

# plot the posterior median covariance function 
curve(median(post$etasq) * exp(-median(post$rhosq) * x^2), from = 0, to = 10, xlab = "distance (thousand km)", ylab = "covariance", ylim = c(0, 1), yaxp = c(0, 1, 4), lwd = 2) 

# plot 100 functions sampled from the posterior 
for (i in 1:100){
  curve(post$etasq[i] * exp(-post$rhosq[i]*x^2), add = TRUE, col = col.alpha("black", 0.2))
}
```

In the plot above we see the posterior distribution of the spatial covariance between pairs of societies. The dark curve displays the posterior median. The thin curves show 100 functions sampled from the joint posterior distribution of eta squared and rho squared. 

Each combination of values for rho squared and eta squared produces a relationship between covariance and distance. The posterior median function, shown by a thick curve, represents a center of plausibility. 

Its hard to interpret these covariances directly, because they are on the log-count scale. just like everything else in a Poisson GLM. So lets consider the correlations among societies that are implied by the posterior median. 

```{r}
# first we push the parameters back through the function for K, the covariance matrix

# compute posterior median covariance among societies 
K <- matrix(0, nrow = 10, ncol = 10) 
for (i in 1:10){
  for (j in 1:10){
    K[i, j] <- median(post$etasq) * exp(-median(post$rhosq) * islandsDistMatrix[i, j]^2)
  }
}
diag(K) <- median(post$etasq) + 0.01 

# convert K to a correlation matrix 
Rho <- round(cov2cor(K), 2) 

# add row / col names for convenience 
colnames(Rho) <- c("Ml", "Ti", "SC", "Ya", "Fi", "Tr", "Ch", "Mn", "To", "Ha") 
rownames(Rho) <- colnames(Rho) 
Rho

```

```{r}
# plot correlations on a map 

# scale point size to logpop 
psize <- d$logpop / max(d$logpop) 
psize <- exp(psize * 1.5) - 2 

# plot raw data and labels 
plot(d$lon2, d$lat, xlab = "longitude", ylab = "latitude", col = rangi2, cex = psize, pch = 16, xlim = c(-50, 30)) 
labels <- as.character(d$culture) 
text(d$lon2, d$lat, labels = labels, cex = 0.7, pos = c(2, 4, 3, 3, 4, 1, 3, 2, 4, 2)) 

# overlay lines shaded by Rho 
for (i in 1:10){
  for (j in 1:10){
    if (i < j){
      lines(c(d$lon2[i], d$lon2[j]), c(d$lat[i], d$lat[j]), lwd = 2, col = col.alpha("black", Rho[i, j]^2))
    }
  }
}

```

In the plot above, darker lines indicate stronger correlations. 

More sense can be made of these correlations, if we also compare against the simultaneous relationshop between tools and log population. Here is a plot that combines the average posterior predictive relationship between log population and total tools with the shaded correlation lines for each pair of societies: 

```{r}
# compute posterior median relationship, ignoring distance 
logpop.seq <- seq(from = 6, to = 14, length.out = 30) 
lambda <- sapply(logpop.seq, function(lp) exp(post$a + post$bp * lp)) 
lambda.median <- apply(lambda, 2, median) 
lambda.PI80 <- apply(lambda, 2, PI, prob = 0.8)

# plot raw data and labels 
plot(d$logpop, d$total_tools, col = rangi2, cex = psize, pch = 16, xlab = "log population", ylab = "total tools")
text(d$logpop, d$total_tools, labels = labels, cex = 0.7, pos = c(4,3,4,2,2,1,4,4,4,2))

# display posterior predictions 
lines(logpop.seq, lambda.median, lty = 2)
lines(logpop.seq, lambda.PI80[1,], lty = 2)
lines(logpop.seq, lambda.PI80[2,], lty = 2)

# overlay correlations 
for (i in 1:10){
  for (j in 1:10){
    if (i < j){
      lines(c(d$logpop[i], d$logpop[j]), 
            c(d$total_tools[i], d$total_tools[j]), 
            lwd = 2, col = col.alpha("black", Rho[i, j]^2))
    }
  }
}
```

