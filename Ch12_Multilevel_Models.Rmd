---
title: "Ch12_Multilevel_Models"
author: "Michael Rose"
date: "July 4, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Multilevel Models 

One of the things that makes multilevel models special is that they remember features of each cluster in the data as they learn about all of the clusters. Depending on the variation among clusters, which is learned from data as well, the model pools information across clusters. This pooling tends to improve estimates about each cluster. 

Some benefits of multilevel models: 
  1. Improved estimates for repeat sampling 
  2. Improved estimates for imbalance in sampling 
  3. Estimates of variation 
  4. Avoid averaging, retain variation 
  
There are costs to the multilevel approach: 
  1. We have to make new assumptions by defining distributions from which the characteristics of the clusters arise. Thankfully, conserbative maximum entropy distributions do an excellent job in this context. 
  2. There are new estimation challenges that come with the full multilevel approach. These lead us into MCMC estimation 
  3. Multilevel models can be hard to understand because they make predictions are different levels of the data. 
  
# 12.1 | Example: Multilevel Tadpoles 

```{r}
library(rethinking)
data(reedfrogs)
d <- reedfrogs
str(d)

```

For now we will be interested in number surviving, surv, out of an initial count - density. 

A multilevel model, in which we simultaneously estimate both an intercept for each tank and the variation among tanks is what we want. This will be a *varying intercepts* model. Varying intercepts are the simplest kind of *varying effects*. For each cluster in the data we use a unique intercept parameter. 

Here is the model for predicting tadpole mortality in each tank, using the regularizing priors of earlier chapters: 
$s_i \sim \mathrm{Binomial}(n_i, p_i)$                # likelihood
$\mathrm{logit}(p_i) = \alpha_{tank[i]}$              # unique log odds for each tank i 
$\alpha_{tank} \sim \mathrm{Normal}(0, 5)$            # weakly regularizing prior 

```{r}
# make the tank cluster variable 
d$tank <- 1:nrow(d)

# fit 
m12.1 <- map2stan(
  alist(
    surv ~ dbinom(density, p), 
    logit(p) <- a_tank[tank], 
    a_tank[tank] ~ dnorm(0, 5)
  ), data = d
)

# inspect estimates
precis(m12.1, depth = 2) 

# get the expected survival probability of a tank 
logistic(-0.67)  #a_tank[46]
```

Now we can fit the multilevel model, which adaptively pools information across tanks. All that is required to enable pooling is to make the prior for the a_tank parameters a function of its own parameters. 

Here is the mathematical form:

$s_i \sim \mathrm{Binomial}(n_i, p_i)$                # likelihood
$\mathrm{logit}(p_i) = \alpha_{tank[i]}$              # unique log odds for each tank i 
$\alpha_{tank} \sim \mathrm{Normal}(\alpha, \sigma)$  # varying intercepts prior 
$\alpha \sim \mathrm{Normal}(0, 1)$                   # prior for average tank
$\sigma \sim \mathrm{HalfCauchy}(0, 1)$               # prior for standard deviation of tanks 

The two parameters for a_tank, alpha and sigma are called *hyperparameters*. They are parameters for parameters. Their priors are often called *hyperpriors*. 

```{r}
# fit model 

m12.2 <- map2stan(
  alist(
    surv ~ dbinom(density, p), 
    logit(p) <- a_tank[tank], 
    a_tank[tank] ~ dnorm(a, sigma), 
    a ~ dnorm(0, 1), 
    sigma ~ dcauchy(0, 1)
  ), data = d, iter = 4000, chains = 4, cores = 4
)
```

```{r}
# compare models with WAIC 
compare(m12.1, m12.2)

```

Things to note in the table above: 
  - The multilevel model has only 38 effective parameters. There are 12 fewer effective parameters than actual parameters because the prior assigned to each intercept shrinks them all towards the mean $\alpha$. In this case, the prior is reasonably strong      and is an example of a *regularizing prior* where the regularization has been learned from the data itself.  
  
To appreciate the impact of this adaptive regularization, we will plot and compare the posterior medians from the models m12.1 and m12.2 

```{r}
# extract stan samples 
post <- extract.samples(m12.2)

# compute median intercept for each tank and transform to probability with logistic 
d$propsurv.est <- logistic(apply(post$a_tank, 2, median)) 

# display raw proportions surviving in each tank 
plot(d$propsurv, ylim = c(0, 1), pch = 16, xaxt = "n", xlab = "tank", ylab = "proportion survival", col = rangi2)
axis(1, at = c(1, 16, 32, 48), labels = c(1, 16, 32, 48)) 

# overlay posterior medians 
points(d$propsurv.est) 

# mark posterior median probability across tanks 
abline(h = logistic(median(post$a)), lty = 2)

# draw vertical dividers between tank densities 
abline(v = 16.5, lwd = 0.5) 
abline(v = 32.4, lwd = 0.5) 
text(8, 0, "small tanks")
text(16+8, 0, "medium tanks") 
text(32+8, 0, "large tanks")

```

In the plot above: 
  - The horizontal axis is tank index, from 1 to 48. 
  - The vertical axis is proportion of survivors in the tank 
  - The filled blue points show the raw proportions, computed from the observed counts 
  - The black circles are the varying intercept medians 
  - The horizontal dashed like at 0.8 is the estimated median survival proportion in the population of tanks (not the same as empirical mean survival)
  
First notice that in every case the multilevel estimate (black circles) is closer to the dashed line than the raw empirical estimate is. This phenomena is known as *shrinkage* and its as if the entire ditribution of black circles has been shrunk towards the dashed line at the center of the data, leaving blue points behind on the outside. 

This phenomena arises from *pooling*. Pooling means that we are pooling information across clusters (tanks) to improve estimates. 

What does the inferred population distribution of survival look like? 

```{r}
# show first 100 populations in the posterior 
plot(NULL, xlim = c(-3, 4), ylim = c(0, 0.35), xlab = "log-odds survive", ylab = "density")

for (i in 1:100){
  curve(dnorm(x, post$a[i], post$sigma[i]), add = TRUE, col = col.alpha("black", 0.2))
}

# sample 8000 imaginary tanks from the posterior distribution 
sim_tanks <- rnorm(8000, post$a, post$sigma)

# transform to probability and visualize 
dens(logistic(sim_tanks), xlab = "probability survive")

```

Above: The interred population of survival across tanks.

# 12.2 | Varying Effects and the Underfitting/Overfitting trade-off 

Varying intercepts are just regularized estimates, but adaptively regularized by estimating how diverse the clusters are while estimating the features of each cluster. A major benefit of varying effect estimates, instead of empirical raw estimates, is that they provide more accurate estimates of the individual cluster (tank) intercepts. 

We can approach our problem of predicting the future survival of the reed frogs from 3 perspectives: 
  1. Complete Pooling: We assume the population of ponds is invariant, then same as estimating a common intercept for all ponds.
  2. No Pooling: We assume that each pond tells us nothing about any other pond. This is the model with amnesia 
  3. Partial Pooling: We use an adaptive regularizing prior
  
We will go over these with simulation.

```{r}
a <- 1.4
sigma <- 1.5
nponds <- 60 
ni <- as.integer(rep(c(5, 10, 25, 35), each = 15)) 
```

We've chosen 60 ponds, with 15 of each initial tadpole density 5, 10, 25, 35. The values 1.4 and 1.5 define a Gaussian distribution of individual pond log odds of survival. 

```{r}
# simulate 60 intercept values from the implied gaussian distribution with mean a and sd sigma 
a_pond <- rnorm(nponds, mean = a, sd = sigma)

# place into data frame
dsim <- data.frame(pond = 1:nponds, ni = ni, true_a = a_pond)

```

## 12.2.3 | Simulate Survivors 

Now we can simulate the binomial survival process. Each pond i has n_i potential survivors, and nature flips each tadpoles coin with probability of survival p_i. This is equal to 
$p_i = \frac{\exp(\alpha_i)}{1 + \exp(\alpha_i)}$ 

Putting the logistic into the random binomial function, we can generate a simulated survivor count for each pond
```{r}
dsim$si <- rbinom(nponds, prob = logistic(dsim$true_a), size = dsim$ni)

# compute the no pooling estimates 
dsim$p_nopool <- dsim$si / dsim$ni 

# compute the partial pooling estimates 
m12.3 <- map2stan(
  alist(
    si ~ dbinom(ni, p), 
    logit(p) <- a_pond[pond], 
    a_pond[pond] ~ dnorm(a, sigma), 
    a ~ dnorm(0, 1), 
    sigma ~ dcauchy(0, 1)
  ), data = dsim, iter = 1e4, warmup = 1000, cores = 4
)

```

```{r}
# look at the estimates for alpha and sigma 
precis(m12.3, depth = 2) 

# compute the predicted survival proportions and add those to our dataframe 
estimated.a_pond <- as.numeric(coef(m12.3)[1:60]) 
dsim$p_partpool <- logistic(estimated.a_pond)

# true per pond survival probabilities using to generate the data 
dsim$p_true <- logistic(dsim$true_a)

# compute the absolute error between the estimates and the true varying effects 
nopool_error <- abs(dsim$p_nopool - dsim$p_true) 
partpool_error <- abs(dsim$p_partpool - dsim$p_true)

# plot 
plot(1:60, nopool_error, xlab = "pond", ylab = "absolute error", col = rangi2, pch = 16)
points(1:60, partpool_error)
```

The plot above shows the error of no pooling estimates for the simulated tadpole ponds. The horizontal axis displays the pond number and the vertical axis measures the absolute error in the predicted proportion of survivors, compared to the true value using simulation. 

Nopooling is shown in blue, and partial pooling is shown in black. The higher the dot, the more error there is. As we can see, partial pooling estimates are better on average. We can also see that the estimates are much better for larger ponds (on the right). This is because more data means better estimates generally. 

#### Overthinking: Repeating the Pond Simulation 

Once we have compiled the mopdel, we cna pass new data to the compiled model and get new estimates. 

```{r}
# resimulate ponds and sample from the new posterior 
a <- 1.4 
sigma <- 1.5 
nponds <- 60
ni <- as.integer(rep(c(5, 10, 25, 35), each = 15)) 
a_pond <- rnorm(nponds, mean = a, sd = sigma) 
dsim <- data.frame(pond = 1:nponds, ni = ni, true_a = a_pond) 
dsim$si <- rbinom(nponds, prob = logistic(dsim$true_a), size = dsim$ni) 
newdat <- list(si = dsim$si, ni = dsim$ni, pond = 1:nponds) 
m12.3new <- map2stan(m12.3, data = newdat, iter = 1e4, warmup = 1000)

```

# 12.3 | More than one type of cluster 

We can use and often should use more than one type of cluster in the same model. For example, the observations in the chimpanzees data are lever pulls. Each pull is within a cluster of pulls belonging to an individual chimpanzee. But each pull is also within an experimental block, which represents a collection of observations that happened on the same day. So each observed pull belongs to both an actor (1:7) and  ablock(1:6). 

## 12.3.1 | Multilevel Chimpanzees 

Here is the multilevel chimpanzees model in mathematical form: 

$L_i \sim \mathrm{Binomial}(1, p_i)$ 
$\mathrm{logit}(p_i) = \alpha + \alpha_{actor[i]} + (\beta_p + \beta_{pc}C_i)P_i$ 
$\alpha_{actor} \sim \mathrm{Normal}(0, \sigma_{actor})$ 
$\alpha \sim \mathrm{Normal}(0, 10)$ 
$\beta_p \sim \mathrm{Normal}(0, 10)$ 
$\beta_{pc} \sim \mathrm{Normal}(0, 10)$ 
$\sigma_{actor} \sim \mathrm{HalfCauchy}(0, 1)$

```{r}
# sample values from two identical Gaussian distributions with mean 10 and sd 1
y1 <- rnorm(1e4, 10, 1)
y2 <- 10 + rnorm(1e4, 0, 1)

dens(y1)
dens(y2)
```

We see from above that y1 and y2 are essentially the same

```{r}
data(chimpanzees)
d <- chimpanzees

d$recipient <- NULL # remove NAs

# fit model 
m12.4 <- map2stan(
  alist(
    pulled_left ~ dbinom(1, p), 
    logit(p) <- a + a_actor[actor] + (bp + bpC * condition)*prosoc_left, 
    a_actor[actor] ~ dnorm(0, sigma_actor), 
    a ~ dnorm(0, 10), 
    bp ~ dnorm(0, 10), 
    bpC ~ dnorm(0, 10), 
    sigma_actor ~ dcauchy(0, 1)
  ), data = d, warmup = 1000, iter = 5000, chains = 4, cores = 4
)

```

```{r}
tracerplot(m12.4)
```

To compute the total intercept for each actor, we need to add samples of a to samples of a_actor

```{r}
post <- extract.samples(m12.4) 
total_a_actor <- sapply(1:7, function(actor) post$a + post$a_actor[, actor]) 
round(apply(total_a_actor, 2, mean), 2)
```

## 12.3.2 | Two types of cluster 

To add the second cluster type, block, we merely replicate the structure for the actor cluster. This means the linear model gets yet another varying intercept, $\alpha_{block[i]}$, and the model gets another adaptive prior and yet another standard deviation parameter. 

$L_i \sim \mathrm{Binomial}(1, p_i)$
$\mathrm{logit}(p_i) = \alpha + \alpha_{actor[i]} + \alpha_{block[i]} + (\beta_p + \beta_{pc}C_i)P_i$ 
$\alpha_{actor} \sim \mathrm{Normal}(0, \sigma_{actor})$
$\alpha_{block[i]} \sim \mathrm{Normal}(0, \sigma_{block})$
$\alpha \sim \mathrm{Normal}(0, 10)$ 
$\beta_p \sim \mathrm{Normal}(0, 10)$ 
$\beta_{pc} \sim \mathrm{Normal}(0, 10)$ 
$\sigma_{actor} \sim \mathrm{HalfCauchy}(0, 1)$
$\sigma_{block} \sim \mathrm{HalfCauchy}(0, 1)$

```{r}
# prep data 
d$block_id <- d$block # name block is reserved by stan 

# fit model 
m12.5 <- map2stan(
  alist(
    pulled_left ~ dbinom(1, p), 
    logit(p) <- a + a_actor[actor] + a_block[block_id] + (bp + bpc*condition)*prosoc_left, 
    a_actor[actor] ~ dnorm(0, sigma_actor), 
    a_block[block_id] ~ dnorm(0, sigma_block), 
    c(a, bp, bpc) ~ dnorm(0, 10), 
    sigma_actor ~ dcauchy(0, 1), 
    sigma_block ~ dcauchy(0, 1)
  ), data = d, warmup = 1000, iter = 6000, chains = 4, cores = 4
)
```

```{r}
tracerplot(m12.5)
```

```{r}
# inspect coefficients 
precis(m12.5, depth = 2) 
plot(precis(m12.5, depth = 2))

```

In the plot above we see the posterior means and 89% density intervals for m12.5 The greater variation across actors than blocks can be seen here.

```{r}
# plot marginal posterior distributions across the 2 sigma parameters 
post <- extract.samples(m12.5) 
dens(post$sigma_block, xlab = "sigma", xlim = c(0, 4))
dens(post$sigma_actor, col = rangi2, lwd = 2, add = TRUE) 
text(2, 0.85, "actor", col = rangi2)
text(0.75, 2, "block")

```

In the plot above we see the posterior distributions of the standard deviations of varying intercepts by actor(blue) and experimental block(black)

```{r}
# compare models
compare(m12.4, m12.5)
```

# 12.4 | Multilevel Posterior Predictions 

```{r}
# compute posterior predictions 
chimp <- 2

d.pred <- list(
  prosoc_left = c(0, 1, 0, 1), # RLRL
  condition = c(0, 0, 1, 1), # control control partner partner
  actor = rep(chimp, 4)
)

link.m12.4 <- link(m12.4, data = d.pred)
pred.p <- apply(link.m12.4, 2, mean)
pred.p.PI <- apply(link.m12.4, 2, PI)

# extract samples 
post <- extract.samples(m12.4)
str(post)

# plot
dens(post$a_actor[,5]) # all samples for actor 5

# to construct posterior predictions, we build our own link function 
p.link <- function(prosoc_left, condition, actor){
  logodds <- with(post, 
                  a + a_actor[, actor] + (bp + bpC * condition) * prosoc_left
                  )
  return(logistic(logodds))
}

# compute predictions 
prosoc_left <- c(0, 1, 0, 1)
condition <- c(0, 0, 1, 1)
pred.raw <- sapply(1:4, function(i) p.link(prosoc_left[i], condition[i], 2)) 
pred.p <- apply(pred.raw, 2, mean) 
pred.p.PI <- apply(pred.raw, 2, PI)
```

## 12.4.2 | Posterior Prediction for new clusters 

Suppose we want to generalize our model from the 7 chimp actors to the entire species. Then we can construct posterior predictions for a now, previously unobserved average actor. 

```{r}
# make a new data list to compute predictions over 
d.pred <- list(
  prosoc_left = c(0,1,0,1), # r l r l
  condition = c(0,0,1,1),   # c c p p
  actor = rep(2, 4)         # placeholder
)

# replace varying intercept samples with zeros. 1000 samples by 7 actors 
a_actor_zeros <- matrix(0, 1000, 7)

# pass the matrix to link using the optional replace argument 
link.m12.4 <- link(m12.4, n = 1000, data = d.pred, replace = list(a_actor = a_actor_zeros)) 

# summarize and plot 
pred.p.mean <- apply(link.m12.4, 2, mean) 
pred.p.PI <- apply(link.m12.4, 2, PI, prob = 0.8) 
plot(0, 0, type = "n", xlab = "prosoc_left / condition", ylab = "proportion pulled left", ylim = c(0, 1), xaxt = "n", xlim = c(1, 4)) 
axis(1, at = 1:4, labels = c("0/0", "1/0", "0/1", "1/1"))
lines(1:4, pred.p.mean)
shade(pred.p.PI, 1:4)
```

In the plot above, the gray region shows the 80% interval for an actor with an average intercept. From this we can see the impact of prosoc_left, as well as the uncertainty of where the average is. It does not show the variation among actors however. 

Setting the varying intercept a_actor to zero produces predictions for an *average* actor. These predictions ifnore uncertainty arising from variation among actors. 

To show the variation among actors, we'll need to use sigma_actor in the calculation. We can smuggle this into like with the replace argument, but this time we will simulate a matrix of new varying intercepts from a Gaussian distribution defined by the adaptive prior in the model itself. 

```{r}
# replace varying intercept samples with simulations 
post <- extract.samples(m12.4)
a_actor_sims <- rnorm(7000, 0, post$sigma_actor)
a_actor_sims <- matrix(a_actor_sims, 1000, 7)

# pass the simulated intercepts into link 
link.m12.4 <- link(m12.4, n = 1000, data = d.pred, replace = list(a_actor = a_actor_sims))

# summarize and plot 
pred.p.mean <- apply(link.m12.4, 2, mean) 
pred.p.PI <- apply(link.m12.4, 2, PI, prob = 0.8) 
plot(0, 0, type = "n", xlab = "prosoc_left / condition", ylab = "proportion pulled left", ylim = c(0, 1), xaxt = "n", xlim = c(1, 4)) 
axis(1, at = 1:4, labels = c("0/0", "1/0", "0/1", "1/1"))
lines(1:4, pred.p.mean)
shade(pred.p.PI, 1:4)

```

In the plot abovE: 
simulating varing intercepts using the posterior standard deviation among actors. 


What we'll do now is write a function that simulates a new actor from the estimated population of actors and then computes probabilities of pulling the left lever for each of the four treatments. These simulations will not average over uncertainty in the posterior. We'll get that unvertainty into the plot by using multiple simulations, each with a different sample from the posterior. 

```{r}
post <- extract.samples(m12.4)
sim.actor <- function(i){
  sim_a_actor <- rnorm(1, 0, post$sigma_actor[i])
  P <- c(0,1,0,1)
  C <- c(0,0,1,1)
  p <- logistic(
    post$a[i] + 
      sim_a_actor + 
      (post$bp[i] + post$bpC[i]*C)*P
  )
  return(p)
}

```

In the function above, i is the index of a sample from the posterior distribution. It draws a random intercept for the actor, using rnorm and a particular value of sigma_actor. Then it computes probabilities p for each of the four treatments, using the same linear model, but with different predictor values inside the P and C vectors. 

```{r}
# plot 50 simulations 

# empty plot 
plot(0, 0, type = "n", xlab = "prosoc_left/condition", ylab = "proportion pulled left", ylim = c(0, 1), xaxt = "n", xlim = c(1, 4)) 
axis(1, at = 1:4, labels = c("0/0", "1/0", "0/1", "1/1"))

# plot 50 simulated actors 
for (i in 1:50) lines(1:4, sim.actor(i), col = col.alpha("black", 0.5))

```

In the plot above: 50 simulated actors with unique intercepts sampled from the posterior. Each simulation maintains the same parameter values across all four treatments. 

## 12.4.3 | Focus and Multilevel Prediction 

Multilevel models contain parameters with different *focus*. In this context, focus means which level of the model the parameter makes direct predictions for. 

It helps to organize the issue into 3 common cases: 
  1. When retroducting the sample, the parameters that describe the population of clusters, such as alpha and sigma_actor do not influence prediction directly. These parameters are called hyperparameters, and they have their effects during estimation, by        shrinking the varying effect parameters towards a common mean. The prediction focus here is on the top level of parameters, not the deeper hyperparameters. 
  2. When forecasting a new observation for a cluster that is present in the sample, we should probably bet the same thing. The focus is on the top level
  3. When we wish to forecast for some new cluster that is not present in the sample, such as a new individual or school or year or location, then we need the hyperparameters. The hyperparameters tell us how to forecast a new cluster, by generating a            distribution of new per cluster intercepts.

When varying effects are used to model overdispersion, this is the right thing to do. In this case, we need to simulate intercepts in order to account for the overdispersion. 

Here is a quick example using the oceanic societies example. 

```{r}
# code for fitting the over dispersed Poisson model 

# prep data 
data(Kline)
d <- Kline
d$log_pop <- log(d$population)
d$society <- 1:10 

# fit model 
m12.6 <- map2stan(
  alist(
    total_tools ~ dpois(mu), 
    log(mu) <- a + a_society[society] + bp*log_pop, 
    a ~ dnorm(0, 10), 
    bp ~ dnorm(0, 1), 
    a_society[society] ~ dnorm(0, sigma_society), 
    sigma_society ~ dcauchy(0, 1)
  ), data = d, iter = 4000, chains = 3, cores = 4
)

```

Now to generate posterior predictions that visualize the overdispersion. We can display posterior predictions by using postcheck, but those predictuins just use the varying intercepts, a_society, directly - they do not use the hyperparameters. 

To instead see the general trend that the model expects, we need to simulate counterfactual societies, using the hyperparameters alpha and sigma_society. 

```{r}
# extract samples 
post <- extract.samples(m12.6) 

# create a grid of predictions 
d.pred <- list(
  log_pop = seq(from = 6, to = 14, length.out = 30), 
  society = rep(1, 30)
)

# create simulations
a_society_sims <- rnorm(20000, 0, post$sigma_society)
a_society_sims <- matrix(a_society_sims, 2000, 10) 

# link function
link.m12.6 <- link(m12.6, n = 2000, data = d.pred, replace = list(a_society = a_society_sims))

# plot raw data 
plot(d$log_pop, d$total_tools, col = rangi2, pch = 16, xlab = "log population", ylab = "total tools") 

# plot posterior median 
mu.median <- apply(link.m12.6, 2, median)
lines(d.pred$log_pop, mu.median) 

# plot 97%, 89%, 67% intervals 
mu.PI <- apply(link.m12.6, 2, PI, prob = 0.97)
shade(mu.PI, d.pred$log_pop)
mu.PI <- apply(link.m12.6, 2, PI, prob = 0.89)
shade(mu.PI, d.pred$log_pop)
mu.PI <- apply(link.m12.6, 2, PI, prob = 0.67)
shade(mu.PI, d.pred$log_pop)
```

In the plot above we see the posterior predictions for the overdispersed poisson island model. 