---
title: "Ch5_Multivariate_Linear_Models"
author: "Michael Rose"
date: "June 16, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
```

Reasons often given for multivariate models include: 
  1. Statistical control for **confounds**. Confounds are variables that may be correlated with another variable of interest. These variables may make it seem like they are important, such as waffle houses and divorce, but end up being noise. 
  2. Multiple Causation. Even without confounds, multiple causes can lead to one effect. 
  3. Interactions. Effective inference about one variable will usually depend upon consideration of other variables. 

This chapter will be focusing on 2 things multivariate models can help us with: 
  1. Revealing *spurious* correlations
  2. Revealing important correlations that may be *masked* by unrevealed correlations with other variables. 
These describe some dangers of multivariate models, notably *multicollinearity*

# 5.1 | Spurious Association 

We are going to model divorce rate with predictors of marriage rate and median age at marriage. The model is as follows: 

$D_i \sim \mathrm{Normal}(\mu_i, \sigma)$
$\mu_i = \alpha + \beta_A A_i$
$\alpha \sim \mathrm{Normal}(10, 10)$
$\beta_A \sim \mathrm{Normal}(0, 1)$
$\sigma \sim \mathrm{Uniform}(0, 10)$

Where $D_i$ is the divorce rate for state i, and $A_i$ is state i's mediag age at marriage. 

```{r}
# load data 
library(rethinking)
data("WaffleDivorce")
d <- WaffleDivorce

# standardize predictor 
d$MedianAgeMarriage.s <- (d$MedianAgeMarriage - mean(d$MedianAgeMarriage)) / sd(d$MedianAgeMarriage)

# fit model 
m5.1 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma), 
    mu <- a + bA * MedianAgeMarriage.s, 
    a ~ dnorm(10, 10), 
    bA ~ dnorm(0, 1), 
    sigma ~ dunif(0, 10)
  ), data = d
)

# compute percentile interval of mean 
MAM.seq <- seq(from = -3, to = 3.5, length.out = 30)
mu <- link(m5.1, data = data.frame(MedianAgeMarriage.s = MAM.seq))
mu.PI <- apply(mu, 2, PI)

# plot everything
plot(Divorce ~ MedianAgeMarriage.s, data = d, col = rangi2)
abline(m5.1)
shade(mu.PI, MAM.seq)

```

When inspecting the precis output, we will see that each additional standard deviation of delay in marriage (1.24 years) predicts a decrease of about one divorce per thousand adults, with an 89% interval from about -1.4 to -0.7 -- so its reliably negative. 

```{r}
precis(m5.1)
```

```{r}
# standardize marriage rate 
d$Marriage.s <- (d$Marriage - mean(d$Marriage)) / sd(d$Marriage)

# fit a regression for the relationship between marriage rate and divorce rate
m5.2 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma), 
    mu <- a + bA * Marriage.s, 
    a ~ dnorm(10, 10), 
    bA ~ dnorm(0, 1), 
    sigma ~ dunif(0, 10)
  ), data = d
)

# compute percentile interval of mean 
mu2 <- link(m5.2, data = data.frame(Marriage.s = MAM.seq))
mu2.PI <- apply(mu2, 2, PI)

# plot 
plot(Divorce ~ Marriage.s, data = d, col = rangi2)
abline(m5.2)
shade(mu2.PI, MAM.seq)

# look at coefficients 
precis(m5.2)

```

When inspecting the precis output for Marriage rate vs divorce rate, we see that we get an increase of 0.6 divorces for every additional standard deviation of marriage rate. 

Comparing the parameter means between different bivariate regressions is no way to decide which predictor is better. These predictors could provide independent value, be redundant, or elimnate the value of the other. So now we are going to build a multivariate model with the goal of measuring the partial value of each predictor, answering the question: 
*What is the predicive value of a variable, once I already know all of the other predictor variables?*

## 5.1.1 | Multivariate Notation 

$D_i \sim \mathrm{Normal}(\mu_i, \sigma)$               Likelihood
$\mu_i = \alpha + \beta_R R_i + \beta_A A_i$            Linear Model 
$\alpha \sim \mathrm{Normal}(10, 10)$                   Prior for $\alpha$
$\beta_R \sim \mathrm{Normal}(0, 1)$                    Prior for $\beta_R$
$\beta_A \sim \mathrm{Normal}(0, 1)$                    Prior for $\beta_A$
$\sigma \sim \mathrm{Uniform}(0, 10)$                   Prior for $\sigma$

Where R is marriage rate and A is age at marriage. 

## 5.1.2 | Fitting the Model 

```{r}
# fit the multivariate model with divorce data 
m5.3 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma), 
    mu <- a + bR * Marriage.s + bA * MedianAgeMarriage.s, 
    a ~ dnorm(10, 10), 
    bR ~ dnorm(0, 1), 
    bA ~ dnorm(0, 1), 
    sigma ~ dunif(0, 10)
  ), data = d
)

precis(m5.3)
plot(precis(m5.3))
```

By our precis above, we can see that the posterior mean for marriage rate, bR is close to 0, the posterior mean for age at marriage, is a bit farther from 0. We can interpret this as the following: 

*Once we know median age at marriage for a state, there is little or no additional predictive power in also knowing the rate of marriage in that state.*

  - As the marriage rate increases it has either a slightly positive or slightly negative effect on divorce
  - As the marriage age increases, we see a lower rate of divorce 

## 5.1.3 | Plotting multivariate posteriors 

Its not as easy to plot multilinear regression plots due to dimensionality, so here are some alternatives: 
  1. Predictor Residual Plots - These plots show the outcome against *residual* predictor variables 
  2. Counterfactual Plots - These show the implied predictions for imaginary experiments in which the different predictor variables can be changed independently of one another
  3. Posterior Prediction Plots - These show model based predictions against raw data, or otherwise display the error in prediction 
  
### 5.1.3.1 | Predictor Residual Plots 

A predictor variable residual is the average prediction error when we use all of the other predictor variables to model a predictor of interest. When a residual is positive, that means that the observed rate was in excess of what we'd expect, given the median age at marriage in that state. When a residual is negative, that means the observed rate was below what we'd expect.

```{r}
# fit a model to plot 
m5.4 <- map(
  alist(
    Marriage.s ~ dnorm(mu, sigma), 
    mu <- a + b*MedianAgeMarriage.s, 
    a ~ dnorm(0, 10), 
    b ~ dnorm(0, 1), 
    sigma ~ dunif(0, 10)
  ), data = d
)

# compute the residuals by subtracting the observed marrage rate in each state from the predicted rate, based upon using age at marriage 

# compute expected value at MAP, for each state 
mu <- coef(m5.4)['a'] + coef(m5.4)['b'] * d$MedianAgeMarriage.s

# compute residual for each state 
m.resid <- d$Marriage.s - mu

# plot 
plot(Marriage.s ~ MedianAgeMarriage.s, d, col = rangi2)
abline(m5.4)
# loop over states
for (i in 1:length(m.resid)){
  x <- d$MedianAgeMarriage.s[i] # x location of line segment
  y <- d$Marriage.s[i] # observed endpoint of line segment 
  # draw the line segment 
  lines(c(x, x), c(mu[i], y), lwd = 0.5, col = col.alpha("black", 0.7))
}

```

From the plot above, we can interpret this in the following way: Residuals above the line are states that have higher rates of marriage given their median age of marriage, and those below the line are those states which have lower rates of marriage given their median age of marriage. 

### 5.1.3.2 | Counterfactual Plots 

This kind of inferential plot displays the implied predictions of the model. They can be produced for any values of the predictor variables that we would like, even unobserved or impossible combinations like very high median age of marriage and very high marriage rate. 

The simplest use of a counterfactual plot is to see how the predictions change as we change only one predictor at a time. This means holding all the predictors constant, except for a single predictor of interest. 

```{r}
# draw a pair of counterfactual plots for the divorce model, beginning with a plot showing the impact of changes in Marriage.s on predictions 

# prepare new counterfactual data 
A.avg <- mean(d$MedianAgeMarriage.s)
R.seq <- seq(from = -3, to = 3, length.out = 30)
pred.data <- data.frame(
  Marriage.s = R.seq, 
  MedianAgeMarriage.s = A.avg
)

# compute counterfactual mean divorce (mu)
mu <- link(m5.3, data = pred.data)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# simulate counterfactual divorce outcomes 
R.sim <- sim(m5.3, data = pred.data, n = 1e4)
R.PI <- apply(R.sim, 2, PI)

# display predictions, hiding raw data with type = "n"
plot(Divorce ~ Marriage.s, data = d, type = "n")
mtext("MedianAgeMarriage.s = 0")
lines(R.seq, mu.mean)
shade(mu.PI, R.seq)
shade(R.PI, R.seq)

```

```{r}
# now making a plot showing the effect of median age on marriages, holding the marriage rate constant 

# prepare new counterfactual data 
R.avg <- mean(d$Marriage.s)
A.seq <- seq(from = -3, to = 3, length.out = 30)
pred.data <- data.frame(
  Marriage.s = R.avg, 
  MedianAgeMarriage.s = A.seq
)

# compute counterfactual mean divorce (mu)
mu <- link(m5.3, data = pred.data)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# simulate counterfactual divorce outcomes 
R.sim <- sim(m5.3, data = pred.data, n = 1e4)
R.PI <- apply(R.sim, 2, PI)

# display predictions, hiding raw data with type = "n"
plot(Divorce ~ MedianAgeMarriage.s, data = d, type = "n")
mtext("Marriage.s = 0")
lines(A.seq, mu.mean)
shade(A.PI, A.seq)
shade(mu.PI, A.seq)
```

### 5.1.3.3 | Posterior Prediction Plots 

We want to know whether the model fit against the observed data was correct. 

```{r}
# call link without specifying new data so it uses original data 
mu <- link(m5.3)

# summarize samples across cases 
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# simulate observations. Again, no new data, so uses original data 
divorce.sim <- sim(m5.3, n = 1e4)
divorce.PI <- apply(divorce.sim, 2, PI)

# plot predictions against observed 
plot(mu.mean ~ d$Divorce, col = rangi2, ylim = range(mu.PI), xlab = "Observed Divorce", ylab = "Predicted Divorce")
abline(a = 0, b = 1, lty = 2)
for (i in 1:nrow(d)){
  lines(rep(d$Divorce[i], 2), c(mu.PI[1, i], mu.PI[2, i]), col = rangi2)
}

# label a few points 
identify(x = d$Divorce, y = mu.mean, labels = d$Loc, cex = 0.8)

```

```{r}
# compute residuals 
divorce.resid <- d$Divorce - mu.mean 

# get ordering by divorce rate 
o <- order(divorce.resid)

# make the plot 
dotchart(divorce.resid[o], labels = d$Loc[o], xlim = c(-6, 5), cex = 0.6)
abline(v = 0, col = col.alpha("black", 0.2))
for (i in 1:nrow(d)){
  j <- o[i] # which state in order 
  lines(d$Divorce[j] - c(mu.PI[1, j], mu.PI[2, j]), rep(i, 2))
  points(d$Divorce[j] - c(divorce.PI[1, j], divorce.PI[2, j]), rep(i, 2), pch = 3, cex = 0.6, col = "gray")
}
```

#### Overthinking | Simulating spurious association 

One way that spurious associations between a predictor and outcome can arise is when a truly causal predictor, $x_{real}$, influences both the outcome, $y$, and a spurious predictor, $x_{spur}$. 
We will simulate this scenario and see how both the spurious data arise and prove to ourselves that multiple regression can reliably indicate the right predictor, $x_{real}$.

```{r}
# simulation 

# number of cases
N <- 100
# x_real as Gaussian with mean 0 and stddev 1
x_real <- rnorm(N) 
# x_spur as Gaussian with mean = x_real 
x_spur <- rnorm(N, x_real)
# y as Gaussian with mean = x_real 
y <- rnorm(N, x_real)
# bind all together in data frame 
d <- data.frame(y, x_real, x_spur) 

pairs(d)

```

Because x_real influences both y and x_spur, we can think of x+spur as another outcome of x_real, but one which we mistake as a potential predictor of y. As a result, both x_spur and x_real are correlated with y. When we include both x variables in a linear regression predicting y, the posterior mean for the association between y and x_spur will be close to zero, while the comparable mean for x_real will be closer to 1. 

# 5.2 | Masked Relationship 

Another reason to use more than one predictor variable is to measure the direct influences of multiple factors on an outcome, when none of those influences is apparent from bivariate relationships. This type of problem arises when we have two predictors that are correlated with one another - but one is positively correlated and the other is negatively correlated. 

```{r}
library(rethinking)
data(milk)
d <- milk
str(d)
```

The analysis for this section is to find out to what extent the energy content of milk, measured in kcol, is related to the percent of the brain mass of the neocortex. 

The first model to consider is the simple bivariate regression between kcal and neocortex percent: 

```{r}
# check for missing data 
d$neocortex.perc

# drop NA values 
dcc <- d[complete.cases(d),]

# fit model to cleaned data
m5.5 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma), 
    mu <- a + bn * neocortex.perc, 
    a ~ dnorm(0, 100), 
    bn ~ dnorm(0, 1), 
    sigma ~ dunif(0, 1)
  ), data = dcc
)

# look at quadratic approximate posterior. add digits = 3 because posterior mean for bn is very small
precis(m5.5, digits = 3)
plot(precis(m5.5))

# plot predicted mean and 89% interval for the mean 
np.seq <- 0:100 
pred.data <- data.frame(neocortex.perc = np.seq)

mu <- link(m5.5, data = pred.data, n = 1e4)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

plot(kcal.per.g ~ neocortex.perc, data = dcc, col = rangi2)
lines(np.seq, mu.mean)
lines(np.seq, mu.PI[1,], lty = 2)
lines(np.seq, mu.PI[2,], lty = 2)

```

Now we will consider another predictor variable, adult female body mass. We will use the log of the mass, since it is often tyue that scaling measurements like body mass are related by magnitudes to other variables. Taking the log of the mass translates this measure into magnitudes. 

```{r}
# transform mass 
dcc$log.mass <- log(dcc$mass)

# fit model 
m5.6 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma), 
    mu <- a + bm*log.mass,
    a ~ dnorm(0, 100), 
    bm ~ dnorm(0, 1),
    sigma ~ dunif(0, 1)
  ), data = dcc
)

precis(m5.6)
plot(precis(m5.6))


```

Now we will add both predictor variables at the same time, so our model is: 

$k_i \sim \mathrm{Normal}(\mu_i, \sigma)$
$\mu_i = \alpha + \beta_n n_i + \beta_m \log(m_i)$
$\alpha \sim \mathrm{Normal}(0, 100)$
$\beta_n \sim \mathrm{Normal}(0, 1)$
$\beta_m \sim \mathrm{Normal}(0, 1)$
$\sigma \sim \mathrm{Uniform}(0, 10)$

```{r}
m5.7 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma), 
    mu <- a + bn * neocortex.perc + bm * log.mass, 
    a ~ dnorm(0, 100),
    bn ~ dnorm(0, 1), 
    bm ~ dnorm(0, 1),
    sigma ~ dunif(0, 1)
  ), data = dcc
)

precis(m5.7)
plot(precis(m5.7))
```

```{r}
# plot the intervals for the predicted mean kcals with a counterfactual plot 
mean.log.mass <- mean(log(dcc$mass))

np.seq <- 0:100 
pred.data <- data.frame(
  neocortex.perc <- np.seq, 
  log.mass = mean.log.mass
)

mu <- link(m5.7, data = pred.data, n = 1e4)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

plot(kcal.per.g ~ neocortex.perc, data = dcc, type = "n")
lines(np.seq, mu.mean)
lines(np.seq, mu.PI[1,], lty = 2)
lines(np.seq, mu.PI[2,], lty = 2)
```

```{r}
# bottom right plot fig 5.7 
lm.seq <- seq(from = -2, to = 4, length.out = 7)
mean.neocortex.perc <- mean(dcc$neocortex.perc)

pred2.data <- data.frame(
  log.mass <- lm.seq, 
  neocortex.perc = mean.neocortex.perc
)

mu <- link(m5.7, data = pred2.data, n = 1e4)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

plot(kcal.per.g ~ log.mass, data = dcc, type = "n")
lines(lm.seq, mu.mean)
lines(lm.seq, mu.PI[1,], lty = 2)
lines(lm.seq, mu.PI[2,], lty = 2)

```

#### Overthinking | Simulating a masking relationship 

We will be simulating data in which two meaningful predictors act to mask one another. 

Suppose we have a single outcome y with two predictors, x_pos and x_neg. x_pos is positively correlated with y, and x_neg negatively.These predictors are also positively correlated with one another. 
```{r}
N <- 100 # number of cases 
rho <- 0.7 # correlation between x_pos and x_neg
x_pos <- rnorm(N) # x pos as Gaussian
x_neg <- rnorm(N, rho * x_pos, sqrt(1 - rho^2)) # xneg correlated with xpos
y <- rnorm(N, x_pos - x_neg) # y equally associated with x_pos, x_neg
d <- data.frame(y, x_pos, x_neg)

pairs(d)
```

# 5.3 | When Adding Variables Hurts 

In this section we will be going over things like multicollinearity, post treatment bias (which means statistically controlling for consequences of a causal factor) and overfitting.

**Multicollinearity** means very strong correlation between two or more predictor variables. 

## 5.3.1 | Multicollinear legs 

This simulation example is predicting an individual's height using the length of his or her legs as predictor variables. 

```{r}
# simulate the height and leg lengths of 100 individuals 
N <- 100                                                 # number of individuals 
height <- rnorm(N, 10, 2)                                # sim total height of each
leg_prop <- runif(N, 0.4, 0.5)                           # leg as proportion of height
leg_left <- leg_prop * height + rnorm(N, 0, 0.02)        # sim left leg as proportion + error
leg_right <- leg_prop * height + rnorm(N, 0, 0.02)       # sim right leg as proportion + error
d <- data.frame(height, leg_left, leg_right)

```

```{r}
# predict height with leg_left and leg_right 
m5.8 <- map(
  alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + bl*leg_left + br*leg_right, 
    a ~ dnorm(10, 100), 
    bl ~ dnorm(2, 10), 
    br ~ dnorm(2, 10), 
    sigma ~ dunif(0, 10)
  ), data = d
)
precis(m5.8)
plot(precis(m5.8))

```

Our model is weird, but fit correctly. Our model answers the following question: 
*What is the value of knowing each predictor, after already knowing all the other predictors?* or in this case: 
*What is the value of knowing each leg's length, after already knowing the others length?*

```{r}
# look at the bivariate posterior distribution for bl and br 
post <- extract.samples(m5.8)
plot(bl ~ br, post, col = col.alpha(rangi2, 0.1), pch = 16)
```

What has happened here is the following: Since both leg variables contain almost exactly the same information, if we insist on including both in a model, there will be a practically infinite number of combinations of bl and br that produce the same predictions. Therefore, we can't really pull the leg variables apart, because they never seperately influence mu. Therefore only their sum influences mu. 

```{r}
# compute the posterior distribution of the sum of legs and plot it 
sum_blbr <- post$bl + post$br
dens(sum_blbr, col = rangi2, lwd = 2, xlab = "sum of bl and br")
```

The posterior mean is a little over 2 and the standard deviation is smaller than it is for either component of the sum, bl or br. On average, an individual's legs are 45% of his or her height. So we should expect a (b1 + b2) coefficient that measures the association of a leg with height to end up around the average height (10) divided by 45% of the average height (4.5), to get 10/4.5 which is approx 2.2. 

```{r}
# If we fit a regression with only one of the leg length variables, we get approximately the same posterior mean
m5.9 <- map(
  alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + bl*leg_left, 
    a ~ dnorm(10, 100), 
    bl ~ dnorm(2, 10), 
    sigma ~ dunif(0, 10)
  ), data = d
)

precis(m5.9)
plot(precis(m5.9))

coef(m5.9)
```

Our bl coef of 2 is close enough to the mean value of sum_blbr. 
*When two predictor variables are very strongly correlated, including both in a model may lead to confusion*

## 5.3.2 | Multicollinear milk 

We will return to the primate milk data for this section. We are addressing the problem that arises in real datasets in which we may not anticipate a clash between highly correlated predictors, and then mistakenly read the posterior distribution to day that neither predictor is important. 

```{r}
d <- milk

# kcal.per.g regressed on perc.fat 
m5.10 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma),
    mu <- a + bf * perc.fat, 
    a ~ dnorm(0.6, 10), 
    bf ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data = d
)

# kcal.per.g regressed on perc.lactose
m5.11 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma), 
    mu <- a + bl * perc.lactose, 
    a ~ dnorm(0.6, 10), 
    bl ~ dnorm(0, 1), 
    sigma ~ dunif(0, 10)
  ), data = d
)

plot(precis(m5.10, digits = 3))
plot(precis(m5.11, digits = 3))
precis(m5.10)
precis(m5.11)
```
The posterior mean for bf (percent fat with milk energy) is 0.01 with 89% interval 0.01 - 0.01. 
The posterior mean for bl (percent lactose with milk energy) is -0.01. 

These two predictors are mirror images of one another. 

```{r}
# try to place both predictor variables in the same regression model 
m5.12 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma), 
    mu <- a + bf * perc.fat + bl * perc.lactose, 
    a ~ dnorm(0.6, 10), 
    bf ~ dnorm(0, 1), 
    bl ~ dnorm(0, 1), 
    sigma ~ dunif(0, 10)
  ), data = d
)

precis(m5.12, digits = 3)
```

The fat and lactose variables contain pretty much the same information. As a result, they form a single axis of variation

```{r}
pairs( ~ kcal.per.g + perc.fat + perc.lactose, data = d, col = rangi2)
```

We can see from the pairs plot above that there is not much variance between perc fat and perc lactose. They are negatively correlated with each other in such a way that they are essentially redundant. Either can help in predicting kcals, but neither helps much once we already know the other. 

```{r}
# correlation
cor(d$perc.fat, d$perc.lactose)
```

The problem of multicollinearity is a member of a family of problems known as **non-identifiability**. When a parameter is non-identifiable, it means that the structure of the data and model do not make it possible to estimate the parameter's value.

#### Overthinking | Simulating collinearity 

This is the code that makes figure 5.10

```{r}
d <- milk
sim.coll <- function(r = 0.9){
  d$x <- rnorm(nrow(d), mean = r* d$perc.fat, sd = sqrt((1 - r^2) * var(d$perc.fat)))
  m <- lm(kcal.per.g ~ perc.fat + x, data = d)
  sqrt(diag(vcov(m)))[2] # stddev of parameter 
}

rep.sim.coll <- function(r = 0.9, n = 100){
  stddev <- replicate(n, sim.coll(r))
  mean(stddev)
}

r.seq <- seq(from = 0, to = 0.99, by = 0.01)
stddev <- sapply(r.seq, function(z) rep.sim.coll(r = z, n = 100))
plot(stddev ~ r.seq, type = "l", col = rangi2, lwd = 2, xlab = "correlation")

```

So for each correlation value in r.seq, the code generates 100 regressions and returns the average standard deviation from them. 

## 5.3.3 | Post-Treatment Bias 

**Omitted Variable Bias** is the type of mistake in which we worry about mistaken inferences that arise from omitting predictor variables. 
**Post Treatment Bias** is worrying about mistaken inferences arising from including variables that are consequences of other variables. 

```{r}
# see what goes wrong when we include a post-treatment variable 

# number of plants
N <- 100

# simulate initial heights 
h0 <- rnorm(N, 10, 2)

# assign treatments and simulate fungus and growth 
treatment <- rep(0:1, each = N/2)
fungus <- rbinom(N, size = 1, prob = 0.5 - treatment * 0.4)
h1 <- h0 + rnorm(N, 5 - 3 * fungus)

# compose a clean data frame 
d <- data.frame(h0 = h0, h1 = h1, treatment = treatment, fungus = fungus)

# fit a model that includes all of the available variables 
m5.13 <- map(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- a + bh*h0 + bt*treatment + bf*fungus, 
    a ~ dnorm(0, 100), 
    c(bh, bt, bf) ~ dnorm(0, 10), 
    sigma ~ dunif(0, 10)
  ), data = d
)

precis(m5.13)

plot(precis(m5.13))
```

From the plot above we see the following: 
  - Our treatment effect is slightly negative 
  - our fungus effect is quite strong in the negative direction 
  - our h0 effect is strong in the positive direction
  
Since we know that treatment matters (we simulated it that way), we can see that something is off. The problem is that fungus is a consequence of treatment, or a *post treatment variable*. When we control for fungus, the model is implicitly answering the question *Once we already know whether or not a plant developed fugus, does soil treatment matter?* The answer is no because soil treatment has its effects on growth through reducing fungus. 

To properly measure the impact of treatment on growth, we should omit the post treatment variable fungus 

```{r}
m5.14 <- map(
  alist(
    h1 ~ dnorm(mu, sigma), 
    mu <- a + bh*h0 + bt*treatment,
    a ~ dnorm(0, 100),
    c(bh, bt) ~ dnorm(0, 10), 
    sigma ~ dunif(0, 10)
  ), data = d
)

precis(m5.14)
plot(precis(m5.14))
```

# 5.4 | Categorical Variables 

## 5.4.1 | Binary Categories 

```{r}
data(Howell1)
d <- Howell1 
str(d)
```

Our model with the dummy variable for maleness: 

$h_i \sim \mathrm{Normal}(\mu_i, \sigma)$
$\mu_i = \alpha + \beta_m m_i$
$\alpha \sim \mathrm{Normal}(178, 100)$
$\beta_m \sim \mathrm{Normal}(0, 10)$
$\sigma \sim \mathrm{Uniform}(0, 50)$

```{r}
# fit the model 
m5.15 <- map(
  alist(
    height ~ dnorm(mu, sigma), 
    mu <- a + bm*male, 
    a ~ dnorm(178, 100), 
    bm ~ dnorm(0, 10), 
    sigma ~ dunif(0, 50)
  ), data = d
)

precis(m5.15)
plot(precis(m5.15))
```

To interpret our estimates, we see that $\alpha$ is the average height amongst females, and the average male height is 7.28 cm taller. 

To derive a percentile interval for average male height, we can sample from the posterior 

```{r}
post <- extract.samples(m5.15) 
mu.male <- post$a + post$bm 
PI(mu.male)
```

#### Overthinking : Reparameterizing the model 

Instead of using a parameter for males and females, we can make our parameters specific to males and females: 

$h_i \sim \mathrm{Normal}(\mu_i, \sigma)$ 
$\mu_i = \alpha_f (1 - m_i) + \alpha_m m_i$ 

Where $\alpha_f$ is the average female height and $\alpha_m$ is the average male height. We can then plug in $\m_i$ = 0 or 1 to verify that either $\alpha_f$ or $\alpha_m$ is turned on for the individual case i. 

```{r}
# fit the model 
m5.15b <- map(
  alist(
    height ~ dnorm(mu, sigma), 
    mu <- af * (1 - male) + am * male, 
    af ~ dnorm(178, 100), 
    am ~ dnorm(178, 100), 
    sigma ~ dunif(0, 50)
  ), data = d
)

```

## 5.4.2 | Many Categories 

The general rule of thumb: for k categories, use k-1 dummy variables. 

```{r}
data(milk)
d <- milk
unique(d$clade)
```

```{r}
# to create dummy variables for the New World Monkey category 
(d$clade.NWM <- ifelse(d$clade == "New World Monkey", 1, 0))

# old world monkey 
(d$clade.OWM <- ifelse(d$clade == "Old World Monkey", 1, 0))

# strepsirrhine 
(d$clade.S <- ifelse(d$clade == "Strepsirrhine", 1, 0))

```
```{r}
m5.16 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma), 
    mu <- a + b.NWM * clade.NWM + b.OWM * clade.OWM + b.S * clade.S, 
    a ~ dnorm(0.6, 10), 
    b.NWM ~ dnorm(0, 1), 
    b.OWM ~ dnorm(0, 1), 
    b.S ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data = d
)

precis(m5.16)
```

The estimate a is the average milk energy for apes, and the estimates for the other categories are differences from apes. 
To get posterior distributions of the average milk energy in each category, we can again use samples from the posterior: 

```{r}
# sample posterior 
post <- extract.samples(m5.16)

# compute averages for each category 
mu.ape <- post$a 
mu.NWM <- post$a + post$b.NWM 
mu.OWM <- post$a + post$b.OWM 
mu.S <- post$a + post$b.S

# summarize using precis 
precis(data.frame(mu.ape, mu.NWM, mu.OWM, mu.S))
```

From the table above, we see that the most plausible (conditional on data and model) average milk energies in each column are 0.55, 0.71, 0.79, and 0.51. 

Once we get accustomed to manipulating estimates this way, we can effectively reparameterize our model after we've already fit it to the data. 

For example, suppose we wanted to know the estimated difference between two monkey groups. Then we can subtract the estimated means and get a difference 

```{r}
diff.NWM.OWM <- mu.NWM - mu.OWM
quantile(diff.NWM.OWM, probs = c(0.025, 0.5, 0.975))
```

## 5.4.3 | Adding Regular Predictor Variables 

We can stack many indicator variables and continuous variables together in a model. 

## 5.4.4 | Another approach: Unique intercepts 

Another way to conceptualize categorical variables is to construct a vector of intercept parameters, one parameter for each category. Then we can create an **index variable** in our data frame that says which parameter goes with each case. 

```{r}
(d$clade_id <- coerce_index(d$clade))
```

This variable gives the number of each unique clade value. There are 4 different clade values. Then we can tell map to make a vector of intercepts, one intercept for each unique value in clade_id 

```{r}
m5.16_alt <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma), 
    mu <- a[clade_id], 
    a[clade_id] ~ dnorm(0.6, 10), 
    sigma ~ dunif(0, 10)
  ), data = d
)

precis(m5.16_alt, depth = 2)
```

# 5.5 | Ordinary Least Squares and lm 

Ordinary Least Squares solves for the parameter values that minimize the sum of the squared residuals. This procedure is often functionally equivalent to maximizing the posterior probability or maximizing the likelihood. Carl Friedrich Gauss invented OLS as a method of computing Bayesian MAP estimates. 

## 5.5.2 | Using lm 

```{r}
# regular fits
m5.17 <- lm(y ~ 1 + x, data = d)
m5.18 <- lm(y ~ 1 + x + z + w, data = d)

# intercepts are optional. These models return the same estimates 
m5.17 <- lm(y ~ 1 + x, data = d)
m5.19 <- lm(y ~ x, data = d)

# When you omit the intercept, lm assumes you wanted one. We can use these forms to make certain there is no intercept 
m5.20 <- lm(y ~ 0 + x, data = d)
m5.21 <- lm(y ~ x - 1, data = d)
```

When using categorical variables, its best to explicitly tell R when we are using a categorical variable as opposed to relying on its innate ability to generate indicator variables 

```{r}
m5.22 <- lm(y ~ 1 + as.factor(season), data = d)
```

When including transformed predictors, its best to transform them before adding them to the lm call 

```{r}
d$x2 <- d$x^2 
d$x3 <- d$x^3 

m5.23 <- lm(y ~ 1 + x + x2 + x3, data = d)

# alternatively we can use I() to make R treat the model as is. We cannot use I() inside map
m5.24 <- lm(y ~ 1 + x + I(x^2) + I(x^3), data = d)
```

We can build map formulas from lm formulas with the rethinking package :

```{r}
data(cars)

glimmer(dist ~ speed, data = cars)
```



