---
title: "Ch5_Multivariate_Linear_Models"
author: "Michael Rose"
date: "June 16, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Reasons often given for multivariate models include: 
  1. Statistical control for **confounds**. Confounds are variables that may be correlated with another variable of interest. These variables may make it seem like they are important, such as waffle houses and divorce, but end up being noise. 
  2. Multiple Causation. Even without confounds, multiple causes can lead to one effect. 
  3. Interactions. Effective inference about one variable will usually depend upon consideration of other variables. 

This chapter will be focusing on 2 things multivariate models can help us with: 
  1. Revealing *spurious* correlations
  2. Revealing important correlations that may be *masked* by unrevealed correlations with other variables. 
These describe some dangers of multivariate models, notably *multicollinearity*

# 5.1 | Spurious Association 

We are going to model divorce rate with predictors of marriage rate and median age at marriage. The model is as follows: 

$D_i \sim \mathrm{Normal}(\mu_i, \sigma)$
$\mu_i = \alpha + \beta_A A_i$
$\alpha \sim \mathrm{Normal}(10, 10)$
$\beta_A \sim \mathrm{Normal}(0, 1)$
$\sigma \sim \mathrm{Uniform}(0, 10)$

Where $D_i$ is the divorce rate for state i, and $A_i$ is state i's mediag age at marriage. 

```{r}
# load data 
library(rethinking)
data("WaffleDivorce")
d <- WaffleDivorce

# standardize predictor 
d$MedianAgeMarriage.s <- (d$MedianAgeMarriage - mean(d$MedianAgeMarriage)) / sd(d$MedianAgeMarriage)

# fit model 
m5.1 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma), 
    mu <- a + bA * MedianAgeMarriage.s, 
    a ~ dnorm(10, 10), 
    bA ~ dnorm(0, 1), 
    sigma ~ dunif(0, 10)
  ), data = d
)

# compute percentile interval of mean 
MAM.seq <- seq(from = -3, to = 3.5, length.out = 30)
mu <- link(m5.1, data = data.frame(MedianAgeMarriage.s = MAM.seq))
mu.PI <- apply(mu, 2, PI)

# plot everything
plot(Divorce ~ MedianAgeMarriage.s, data = d, col = rangi2)
abline(m5.1)
shade(mu.PI, MAM.seq)

```

When inspecting the precis output, we will see that each additional standard deviation of delay in marriage (1.24 years) predicts a decrease of about one divorce per thousand adults, with an 89% interval from about -1.4 to -0.7 -- so its reliably negative. 

```{r}
precis(m5.1)
```

```{r}
# standardize marriage rate 
d$Marriage.s <- (d$Marriage - mean(d$Marriage)) / sd(d$Marriage)

# fit a regression for the relationship between marriage rate and divorce rate
m5.2 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma), 
    mu <- a + bA * Marriage.s, 
    a ~ dnorm(10, 10), 
    bA ~ dnorm(0, 1), 
    sigma ~ dunif(0, 10)
  ), data = d
)

# compute percentile interval of mean 
mu2 <- link(m5.2, data = data.frame(Marriage.s = MAM.seq))
mu2.PI <- apply(mu2, 2, PI)

# plot 
plot(Divorce ~ Marriage.s, data = d, col = rangi2)
abline(m5.2)
shade(mu2.PI, MAM.seq)

# look at coefficients 
precis(m5.2)

```

When inspecting the precis output for Marriage rate vs divorce rate, we see that we get an increase of 0.6 divorces for every additional standard deviation of marriage rate. 

Comparing the parameter means between different bivariate regressions is no way to decide which predictor is better. These predictors could provide independent value, be redundant, or elimnate the value of the other. So now we are going to build a multivariate model with the goal of measuring the partial value of each predictor, answering the question: 
*What is the predicive value of a variable, once I already know all of the other predictor variables?*

## 5.1.1 | Multivariate Notation 

$D_i \sim \mathrm{Normal}(\mu_i, \sigma)$               Likelihood
$\mu_i = \alpha + \beta_R R_i + \beta_A A_i$            Linear Model 
$\alpha \sim \mathrm{Normal}(10, 10)$                   Prior for $\alpha$
$\beta_R \sim \mathrm{Normal}(0, 1)$                    Prior for $\beta_R$
$\beta_A \sim \mathrm{Normal}(0, 1)$                    Prior for $\beta_A$
$\sigma \sim \mathrm{Uniform}(0, 10)$                   Prior for $\sigma$

Where R is marriage rate and A is age at marriage. 

## 5.1.2 | Fitting the Model 

```{r}
# fit the multivariate model with divorce data 
m5.3 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma), 
    mu <- a + bR * Marriage.s + bA * MedianAgeMarriage.s, 
    a ~ dnorm(10, 10), 
    bR ~ dnorm(0, 1), 
    bA ~ dnorm(0, 1), 
    sigma ~ dunif(0, 10)
  ), data = d
)

precis(m5.3)
plot(precis(m5.3))
```

By our precis above, we can see that the posterior mean for marriage rate, bR is close to 0, the posterior mean for age at marriage, is a bit farther from 0. We can interpret this as the following: 

*Once we know median age at marriage for a state, there is little or no additional predictive power in also knowing the rate of marriage in that state.*

  - As the marriage rate increases it has either a slightly positive or slightly negative effect on divorce
  - As the marriage age increases, we see a lower rate of divorce 

## 5.1.3 | Plotting multivariate posteriors 

Its not as easy to plot multilinear regression plots due to dimensionality, so here are some alternatives: 
  1. Predictor Residual Plots - These plots show the outcome against *residual* predictor variables 
  2. Counterfactual Plots - These show the implied predictions for imaginary experiments in which the different predictor variables can be changed independently of one another
  3. Posterior Prediction Plots - These show model based predictions against raw data, or otherwise display the error in prediction 
  
### 5.1.3.1 | Predictor Residual Plots 

A predictor variable residual is the average prediction error when we use all of the other predictor variables to model a predictor of interest. When a residual is positive, that means that the observed rate was in excess of what we'd expect, given the median age at marriage in that state. When a residual is negative, that means the observed rate was below what we'd expect.

```{r}
# fit a model to plot 
m5.4 <- map(
  alist(
    Marriage.s ~ dnorm(mu, sigma), 
    mu <- a + b*MedianAgeMarriage.s, 
    a ~ dnorm(0, 10), 
    b ~ dnorm(0, 1), 
    sigma ~ dunif(0, 10)
  ), data = d
)

# compute the residuals by subtracting the observed marrage rate in each state from the predicted rate, based upon using age at marriage 

# compute expected value at MAP, for each state 
mu <- coef(m5.4)['a'] + coef(m5.4)['b'] * d$MedianAgeMarriage.s

# compute residual for each state 
m.resid <- d$Marriage.s - mu

# plot 
plot(Marriage.s ~ MedianAgeMarriage.s, d, col = rangi2)
abline(m5.4)
# loop over states
for (i in 1:length(m.resid)){
  x <- d$MedianAgeMarriage.s[i] # x location of line segment
  y <- d$Marriage.s[i] # observed endpoint of line segment 
  # draw the line segment 
  lines(c(x, x), c(mu[i], y), lwd = 0.5, col = col.alpha("black", 0.7))
}

```

From the plot above, we can interpret this in the following way: Residuals above the line are states that have higher rates of marriage given their median age of marriage, and those below the line are those states which have lower rates of marriage given their median age of marriage. 

### 5.1.3.2 | Counterfactual Plots 

This kind of inferential plot displays the implied predictions of the model. They can be produced for any values of the predictor variables that we would like, even unobserved or impossible combinations like very high median age of marriage and very high marriage rate. 

The simplest use of a counterfactual plot is to see how the predictions change as we change only one predictor at a time. This means holding all the predictors constant, except for a single predictor of interest. 

```{r}
# draw a pair of counterfactual plots for the divorce model, beginning with a plot showing the impact of changes in Marriage.s on predictions 

# prepare new counterfactual data 
A.avg <- mean(d$MedianAgeMarriage.s)
R.seq <- seq(from = -3, to = 3, length.out = 30)
pred.data <- data.frame(
  Marriage.s = R.seq, 
  MedianAgeMarriage.s = A.avg
)

# compute counterfactual mean divorce (mu)
mu <- link(m5.3, data = pred.data)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# simulate counterfactual divorce outcomes 
R.sim <- sim(m5.3, data = pred.data, n = 1e4)
R.PI <- apply(R.sim, 2, PI)

# display predictions, hiding raw data with type = "n"
plot(Divorce ~ Marriage.s, data = d, type = "n")
mtext("MedianAgeMarriage.s = 0")
lines(R.seq, mu.mean)
shade(mu.PI, R.seq)
shade(R.PI, R.seq)

```

```{r}
# now making a plot showing the effect of median age on marriages, holding the marriage rate constant 

# prepare new counterfactual data 
R.avg <- mean(d$Marriage.s)
A.seq <- seq(from = -3, to = 3, length.out = 30)
pred.data <- data.frame(
  Marriage.s = R.avg, 
  MedianAgeMarriage.s = A.seq
)

# compute counterfactual mean divorce (mu)
mu <- link(m5.3, data = pred.data)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# simulate counterfactual divorce outcomes 
R.sim <- sim(m5.3, data = pred.data, n = 1e4)
R.PI <- apply(R.sim, 2, PI)

# display predictions, hiding raw data with type = "n"
plot(Divorce ~ MedianAgeMarriage.s, data = d, type = "n")
mtext("Marriage.s = 0")
lines(A.seq, mu.mean)
shade(A.PI, A.seq)
shade(mu.PI, A.seq)
```

### 5.1.3.3 | Posterior Prediction Plots 

We want to know whether the model fit against the observed data was correct. 

```{r}
# call link without specifying new data so it uses original data 
mu <- link(m5.3)

# summarize samples across cases 
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# simulate observations. Again, no new data, so uses original data 
divorce.sim <- sim(m5.3, n = 1e4)
divorce.PI <- apply(divorce.sim, 2, PI)

# plot predictions against observed 
plot(mu.mean ~ d$Divorce, col = rangi2, ylim = range(mu.PI), xlab = "Observed Divorce", ylab = "Predicted Divorce")
abline(a = 0, b = 1, lty = 2)
for (i in 1:nrow(d)){
  lines(rep(d$Divorce[i], 2), c(mu.PI[1, i], mu.PI[2, i]), col = rangi2)
}

# label a few points 
identify(x = d$Divorce, y = mu.mean, labels = d$Loc, cex = 0.8)

```

```{r}
# compute residuals 
divorce.resid <- d$Divorce - mu.mean 

# get ordering by divorce rate 
o <- order(divorce.resid)

# make the plot 
dotchart(divorce.resid[o], labels = d$Loc[o], xlim = c(-6, 5), cex = 0.6)
abline(v = 0, col = col.alpha("black", 0.2))
for (i in 1:nrow(d)){
  j <- o[i] # which state in order 
  lines(d$Divorce[j] - c(mu.PI[1, j], mu.PI[2, j]), rep(i, 2))
  points(d$Divorce[j] - c(divorce.PI[1, j], divorce.PI[2, j]), rep(i, 2), pch = 3, cex = 0.6, col = "gray")
}
```

#### Overthinking | Simulating spurious association 

One way that spurious associations between a predictor and outcome can arise is when a truly causal predictor, $x_{real}$, influences both the outcome, $y$, and a spurious predictor, $x_{spur}$. 
We will simulate this scenario and see how both the spurious data arise and prove to ourselves that multiple regression can reliably indicate the right predictor, $x_{real}$.

```{r}
# simulation 

# number of cases
N <- 100
# x_real as Gaussian with mean 0 and stddev 1
x_real <- rnorm(N) 
# x_spur as Gaussian with mean = x_real 
x_spur <- rnorm(N, x_real)
# y as Gaussian with mean = x_real 
y <- rnorm(N, x_real)
# bind all together in data frame 
d <- data.frame(y, x_real, x_spur) 

pairs(d)

```

Because x_real influences both y and x_spur, we can think of x+spur as another outcome of x_real, but one which we mistake as a potential predictor of y. As a result, both x_spur and x_real are correlated with y. When we include both x variables in a linear regression predicting y, the posterior mean for the association between y and x_spur will be close to zero, while the comparable mean for x_real will be closer to 1. 

# 5.2 | Masked Relationship 

Another reason to use more than one predictor variable is to measure the direct influences of multiple factors on an outcome, when none of those influences is apparent from bivariate relationships. 

```{r}
library(rethinking)
data(milk)
d <- milk
str(d)
```

