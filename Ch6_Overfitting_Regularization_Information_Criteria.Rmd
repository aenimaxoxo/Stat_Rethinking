---
title: "Ch6_Overfitting_Regularization_Information_Criteria"
author: "Michael Rose"
date: "June 19, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
```

# 6.1 | The problem with parameters 

```{r}
sppnames <- c("afarensis", "africanus", "habilis", "boisei", "rudolfensis", "ergaster", "sapiens")
brainvolcc <- c(438, 452, 612, 521, 752, 871, 1350)
masskg <- c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)
d <- data.frame(species = sppnames, brain = brainvolcc, mass = masskg)
d
```

The simplest model that relates brain size to body size is the linear one. 
$v_i \sim \mathrm{Normal}(\mu_i, \sigma)$
$\mu_i = \alpha + \beta_1 m_i$

```{r}
# fit a linear model
m6.1 <- lm(brain ~ mass, data = d)

# find R^2 
rsquared <- 1 - var(resid(m6.1)) / var(d$brain)

# parabola 
m6.2 <- lm(brain ~ mass + I(mass^2), data = d)

# cubic 
m6.3 <- lm(brain ~ mass + I(mass^2) + I(mass^3), data = d)

# quartic 
m6.4 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4), data = d)

# quintic 
m6.5 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5), data = d)

# sextic 
m6.6 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6), data = d)

# flat underfit 
m6.7 <- lm(brain ~ 1, data = d)
```

#### Overthinking | Dropping Rows 

```{r}
# drop a row, i, from a dataframe d 
d.new <- d[-i,]

# repeat the regression 
plot(brain ~ mass, d, col = "slateblue")
for (i in 1:nrow(d)){
  d.new <- d[-i,]
  m0 <- lm(brain ~ mass, d.new)
  abline(m0, col = col.alpha("black", 0.5))
}
```

# 6.2 | Information Theory and Model Performance 

We must choose a target for our models before we use things like regularization or information criteria. Once we have a target, we can use information theory to provide a common and useful target, the *out of sample deviance*. 

**First** we must establish the joint probability, not average probability is the right way to judge model accuracy 

**Second** we need to establish a measurement scale for distance from perfect accuracy. This will require information theory because it provides a natural measurement of scale for the distance between two probability distributions. 

**Third** we need to establish deviance as an approximation of relative distance from perfect accuracy. 

*Finally* we must establish that it is *only* deviance of our sample that is of interest. 

In defining a target, there are two dimensions to worry about: 
  1. *Cost Benefit Analysis* - How much does it cost when we are wrong? How much is there to gain from accuracy? 
  2. *Accuracy in Context* - We need to choose an accuracy metric relative to the prediction task at hand's difficulty 

The key insight to information theory is this: *How much is our uncertainty reduced by learning an outcome?* Then our definition: 
*Information* - The reduction in uncertainty derived from learning an outcome 

**Information Entropy**: $H(p) = - \mathrm{E}\log(p_i) = - \sum_{i = 1}^{n} p_i \log(p_i)$. In plain words, *The uncertainty contained in a probability distribution is the average log-probability of an event*.

```{r}
# Information Entropy 
p <- c(0.3, 0.7)
-sum(p * log(p))

# suppose we lived in abu dhabi where p rain = 0.01 and p shine = 0.99 
p <- c(0.01, 0.99)
-sum(p * log(p))

# sun rain snow 
p <- c(0.7, 0.15, 0.15)
-sum(p * log(p))
```

## 6.2.3 | From entropy to accuracy 

*Divergence* - The additional uncertainty induced by using probabilities from one distribution to another distribution. 

This is often know as the *Kullback-Leibler Divergence* or *K-L Divergence*. 

$D_{KL}(p, q) = \sum_{i} p_i (\log(p_i) - log(q_i)) = \sum_{i} \log(\frac{p_i]}{q_i})$ 
Or *The divergence is the average difference in log probability between the target (p) and the model (q)*.
The divergence is just the difference between two entropies. 

The divergence allows us to contrast between different approximations to p. As an approximating function q becomes more accurate, $D_{KL}$ will shrink. 

## 6.2.4 | From Divergence to Deviance 

Information Theory gives us the distance measure we need, K-L divergence, to measure the distance between our model and a target. 
We also now need to know how to estimate the divergence, having identified the right measure of distance. 

Divergence leads to a measure of model fit known as *Deviance*. Deviance is a very common measure of relative fit, which approximates the relative value of $\mathrm{E} \log(q_i)$, defined as $D(q) = -2 \sum_{i}\log(q_i)$ 


```{r}
# fit model with lm 
m6.1 <- lm(brain ~ mass, d)

# compute deviance by cheating 
(-2) * logLik(m6.1)
```

#### Overthinking | Computing Deviance 

```{r}
# standardize the mass before fitting 
d$mass.s <- (d$mass - mean(d$mass)) / sd(d$mass)

# fit 
m6.8 <- map(
  alist(
    brain ~ dnorm(mu, sigma), 
    mu <- a + b*mass.s
  ), data = d,
  start = list(a = mean(d$brain), b = 0, sigma = sd(d$brain)),
  method = "Nelder-Mead"
)

# extract MAP estimates 
theta <- coef(m6.8)

# compute deviance 
(dev <- (-2) * sum(dnorm(
  d$brain, 
  mean = theta[1] + theta[2] * d$mass.s, 
  sd = theta[3],
  log = TRUE
)))

```

## 6.2.5 | From deviance to out of sample

We can essentially do cross validation to see how we are under / overfitting our models. 

# 6.3 | Regularization 

One way to prevent overfitting is to give a model a skeptical prior. The most common skeptical prior is a **Regularizing Prior**. Essentially, we can restrict the learning rate on training data with conservative priors, allowing us to prevent overfitting on the validation sample. 

# 6.4 | Information Criteria 

The difference between the training and validation set points is roughly double the number of parameters. This is the phenomenon behind **information criteria**. The most well know information criteria is the **Akaike Information Criterion**.

AIC provides a simple estimate of the average out of sample deviance: $\mathrm{AIC} = D_{train} + 2p$. 

AIC is an approximation that is reliable only when: 

  1. The priors are flat or overwhelmed by likelihood. 
  2. The posterior distribution is approximately multivariate gaussian. 
  3. The sample size N is much greater than the number of parameters k 

Other Information Criteria: 
**DIC: Deviance Information Criterion** - Accomodates informative priors, still assumes posterior is multivariate gaussian and that N >> k
**WAIC: Widely Applicable Information Criterion** - makes no assumption about the shape of the posterior. 

#### Overthinking | WAIC Calculations 

```{r}
# load data 
data(cars)

# fit model 
m6.9 <- map(
  alist(
    dist ~ dnorm(mu, sigma), 
    mu <- a + b*speed, 
    a ~ dnorm(0, 100), 
    b ~ dnorm(0, 10), 
    sigma ~ dunif(0, 30)
  ), data = cars
)

# sample from posterior 
post <- extract.samples(m6.9, n = 1000)

# get log likelihood of each observation i at each sample s from posterior 
n_samples <- 1000

ll <- sapply(1:n_samples, function(s){
  mu <- post$a[s] + post$b[s] * cars$speed
  dnorm(cars$dist, mu, post$sigma[s], log = TRUE)
})

# we get a 50x1000 matrix of log likelihoods. 
# now we compute lppd by averaging samples in each row, taking the log, and adding the logs together 
n_cases <- nrow(cars)
lppd <- sapply(1:n_cases, function(i) log_sum_exp(ll[i,]) - log(n_samples))

# look at lppd 
sum(lppd)

# calculate effective number of parameters, pwaic 
pWAIC <- sapply(1:n_cases, function(i) var(ll[i,]))

# look at pwaic
sum(pWAIC)

# compute WAIC
(WAIC_fin <- (-2)*(sum(lppd) - sum(pWAIC)))

# compare to WAIC function 
waic_vec <- -2*(lppd - pWAIC)
sqrt(n_cases * var(waic_vec))
sum(waic_vec)

```

# 6.5 | Using Information Criteria 

**Model Comparison** means using DIC/WAIC in combination with the estimates and posterior predictive checks from each model. 
**Model Averaging** means using DIC/WAIC to construct a posterior predictive distribution that explots what we know about relative accuracy of the models. 

## 6.5.1 | Model Comparison 

```{r}
# load data and remove NAs
data(milk)
d <- milk[complete.cases(milk),]

# standardize neocortex percent 
d$neocortex <- d$neocortex.perc / 100 
dim(d)

```

It is important that we remove incomplete cases (those containing any missing values), because we must make sure that *compared models fit to exactly the same observations*.

```{r}
# fit 4 models. Use trick to make sigma positive with ln(e(sigma))

# define starting points for optim 
a.start <- mean(d$kcal.per.g)
sigma.start <- log(sd(d$kcal.per.g))

m6.11 <- map(
  alist(
    kcal.per.g ~ dnorm(a, exp(log.sigma))
  ), data = d, start = list(a = a.start, log.sigma = sigma.start)
)

m6.12 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, exp(log.sigma)), 
    mu <- a + bn*neocortex
  ), data = d, start = list(a = a.start, bn = 0, log.sigma = sigma.start)
)

m6.13 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, exp(log.sigma)), 
    mu <- a + bm*log(mass)
  ), data = d, start = list(a = a.start, bm = 0, log.sigma = sigma.start)
)

m6.14 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, exp(log.sigma)), 
    mu <- a + bn*neocortex + bm*log(mass)
  ), data = d, start = list(a = a.start, bn = 0, bm = 0, log.sigma = sigma.start)
)

```

### 6.5.1.1 | Comparing WAIC values 

To compare models using an information criterion, we must first compute the criterion. Then we can rank models from lowest (best) to worst (highest) and calculate weights, which provide a more interpretable measure of the relative distances among the models. 

```{r}
# compute WAIC
WAIC(m6.14)

# rank models by WAIC 
(milk.models <- compare(m6.11, m6.12, m6.13, m6.14))
```

*WAIC* is the WAIC for each model. In this case, m6.14 is the lowest (-15.4), so it is ranked first 
*pWAIC* is the estimated effective number of parameters. This provides a clue as to how flexible each model is in fitting the sample 
*dWAIC* is the difference between each WAIC and the lowest WAIC. 
*weight* is the Akaike weight for each model. A model's weight is an estimate of the probability that the model will make the best predictions on the new data, conditional on the set of models considered. 
*SE* is the standard error of the WAIC estimate. The uncertainty of the model is well approximated by the standard error. 
*dSE* is the se of the difference between the best model and the current model. 

```{r}
# plot 
plot(milk.models, SE = TRUE, dSE = TRUE)
```

### 6.5.1.2 | Comparing Estimates 

In addition to comparing models on the basis of expected test deviance, it is also useful to compare paramter estimates among models. 

```{r}
# the coeftab function takes a series of fit models as input and builds a table that compares models with different combinations of parameters 
coeftable <- coeftab(m6.11, m6.12, m6.13, m6.14)

plot(coeftable)
```

## 6.5.2 | Model Averaging 

```{r}
# plot counterfactual predictions with neocortex from 0.5 to 0.8 

# create neocortex sequence
nc.seq <- seq(from = 0.5, to = 0.8, length.out = 30)

# predictor list 
d.predict <- list(
  kcal.per.g = rep(0, 30), # empty outcome 
  neocortex = nc.seq,      # sequence of neocortex
  mass = rep(4.5, 30)      # average mass
)

# link function, computes mu for each sample in the posterior
pred.m614 <- link(m6.14, data = d.predict)

# apply mean to link function 
mu <- apply(pred.m614, 2, mean)

# apply probability interval to link function
mu.PI <- apply(pred.m614, 2, PI)

# plot it all 
plot(kcal.per.g ~ neocortex, d, col = rangi2)
lines(nc.seq, mu, lty = 2)
lines(nc.seq, mu.PI[1,], lty = 2)
lines(nc.seq, mu.PI[2,], lty = 2)

```

Above we have a plot which shows the relationship between neocortex percent and kcal.per.g of milk according to model 6.14. 
Now we will compute and add model averaged posterior predictions, through computing an **ensemble** of posterior predictions. 

Conceptual Procedure: 
  1. Compute WAIC (or other information criterion) for each model
  2. Compute the weight for each model 
  3. Compute linear model and simulated outcomes for each model 
  4. Combine these values into an esnemble of predictions, using the model weights as proportions. 
  
We will do this with the ensemble function, which simply calls link and sim for each model we give it. It then combines the results according to Akaike weights.

```{r}
# build an ensemble according to WAIC weight 
milk.ensemble <- ensemble(m6.11, m6.12, m6.13, m6.14, data = d.predict)

# add means 
mu <- apply(milk.ensemble$link, 2, mean)

# add probability intervals 
mu.PI <- apply(milk.ensemble$link, 2, PI)

# plot 
plot(kcal.per.g ~ neocortex, d, col = rangi2)
lines(nc.seq, mu, lty = 2)
lines(nc.seq, mu.PI[1,], lty = 2)
lines(nc.seq, mu.PI[2,], lty = 2)
lines(nc.seq, mu)
shade(mu.PI, nc.seq)

```

The regression line has hardly moved because m6.14 had 90% of the weight. Model ensembling did have a big effect on the intervals of $\mu$.